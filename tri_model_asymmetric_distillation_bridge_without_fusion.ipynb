{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Two-Stage Contextual Bridge Distillation — Sports-in-the-Wild\n",
    "\n",
    "This notebook now implements a **sequential 2-stage knowledge bridge** instead of training all three models jointly. The idea:\n",
    "\n",
    "```\n",
    "Teacher (Base)  ──►  Stage 1: Train Small  ──►  Stage 2: Small (frozen) teaches Tiny\n",
    "        (global rich knowledge)              (domain-adapted bridge)         (final efficient student)\n",
    "```\n",
    "\n",
    "Why a contextual bridge?\n",
    "- The direct gap Teacher(Base) → Student(Tiny) can be large (capacity + representation mismatch).\n",
    "- First adapting an intermediate (Small) yields a *domain-specialized assistant*.\n",
    "- Then the Tiny model learns from both: retained high-level signal (Teacher) + distilled, compressed domain signal (Assistant).\n",
    "\n",
    "Outcome: Better stability + improved tiny accuracy vs single-hop distillation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Flow Overview\n",
    "\n",
    "We run **two independent training jobs** inside one notebook:\n",
    "\n",
    "### Stage 1: Teacher → Assistant (Train ViT-Small)\n",
    "Goal: Produce a strong, domain-adapted SMALL model that will act as a frozen assistant in Stage 2.\n",
    "\n",
    "Configuration principles:\n",
    "- Student model = `videomae-small` (becomes the assistant later)\n",
    "- Assistant influence weights = 0 (no assistant yet)\n",
    "- Only Teacher → Student logits (and optionally features) KD\n",
    "\n",
    "### Stage 2: Assistant → Student (Train ViT-Tiny)\n",
    "Goal: Train the TINY model using BOTH the frozen Small (assistant) and the original Teacher.\n",
    "\n",
    "Configuration principles:\n",
    "- Assistant model path = checkpoint directory produced in Stage 1\n",
    "- Teacher still provides a small stabilizing signal\n",
    "- Assistant has higher logits weight (primary mentor)\n",
    "\n",
    "### Advantages of This Design\n",
    "- Reduces representational jump distance\n",
    "- Lets the Assistant internalize domain specifics before mentoring Tiny\n",
    "- Often yields +accuracy versus a direct Teacher→Tiny pipeline\n",
    "\n",
    "Proceed through sections in order. Skip Stage 1 only if you already have a trained Small checkpoint you want to reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n",
      "CUDA available: True\n",
      "torch: 2.1.0+cu118 cuda: 11.8\n",
      "torchvision: 0.16.0+cu118\n",
      "pytorchvideo: 0.1.5\n",
      "has functional_tensor: True\n"
     ]
    }
   ],
   "source": [
    "# Environment / Common Imports\n",
    "import os, json, torch\n",
    "import torchvision, pytorchvideo, transformers\n",
    "from datetime import datetime\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from tri_model_distillation.config import TriModelConfig\n",
    "from tri_model_distillation.models import TriModelDistillationFramework\n",
    "from tri_model_distillation.trainer import TriModelDistillationTrainer, compute_video_classification_metrics\n",
    "from tri_model_distillation.utils import (\n",
    "    setup_logging, load_label_mappings, create_data_loaders,\n",
    ")\n",
    "\n",
    "print(torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"pytorchvideo:\", pytorchvideo.__version__)\n",
    "print(\"has functional_tensor:\", hasattr(__import__('torchvision.transforms', fromlist=['']), 'functional_tensor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./contextual_bridge_runs_20250828_151346\n",
      "Detected 30 classes\n",
      "['archery', 'baseball', 'basketball', 'bmx', 'bowling', 'boxing', 'cheerleading', 'discusthrow', 'diving', 'football', 'golf', 'gymnastics', 'hammerthrow', 'highjump', 'hockey', 'hurdling', 'javelin', 'longjump', 'polevault', 'rowing', 'running', 'shotput', 'skating', 'skiing', 'soccer', 'swimming', 'tennis', 'volleyball', 'weight', 'wrestling'] ...\n"
     ]
    }
   ],
   "source": [
    "# Dataset + Label Mapping\n",
    "DATASET_ROOT = 'processed_dataset'\n",
    "BASE_RUN_DIR = f\"./contextual_bridge_runs_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "STAGE1_OUTPUT_DIR = f\"{BASE_RUN_DIR}/stage1_teacher_to_small\"\n",
    "STAGE2_OUTPUT_DIR = f\"{BASE_RUN_DIR}/stage2_small_to_tiny\"\n",
    "os.makedirs(STAGE1_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(STAGE2_OUTPUT_DIR, exist_ok=True)\n",
    "print(BASE_RUN_DIR);\n",
    "\n",
    "label2id, id2label = load_label_mappings(dataset_root=DATASET_ROOT, train_csv='train.csv', classification_type='multiclass')\n",
    "num_labels = len(label2id)\n",
    "print(f\"Detected {num_labels} classes\")\n",
    "print(list(label2id.keys())[:30], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Stage 1 checkpoint detected in ./contextual_bridge_runs_20250828_151346/stage1_teacher_to_small. Stage 1 training will be skipped.\n",
      "(Delete or rename the directory to force retraining.)\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 Skip Logic — Detect existing checkpoint to optionally skip Stage 1 training\n",
    "import os, glob\n",
    "stage1_checkpoint_exists = any(\n",
    "    os.path.exists(os.path.join(STAGE1_OUTPUT_DIR, fname))\n",
    "    for fname in ['pytorch_model.bin', 'model.safetensors', 'config.json']\n",
    ")\n",
    "if not stage1_checkpoint_exists:\n",
    "    for sub in glob.glob(os.path.join(STAGE1_OUTPUT_DIR, 'checkpoint-*')):\n",
    "        if os.path.exists(os.path.join(sub, 'pytorch_model.bin')) or os.path.exists(os.path.join(sub, 'model.safetensors')):\n",
    "            stage1_checkpoint_exists = True\n",
    "            break\n",
    "\n",
    "SKIP_STAGE1 = stage1_checkpoint_exists\n",
    "if SKIP_STAGE1:\n",
    "    print(f\"Existing Stage 1 checkpoint detected in {STAGE1_OUTPUT_DIR}. Stage 1 training will be skipped.\")\n",
    "    print(\"(Delete or rename the directory to force retraining.)\")\n",
    "else:\n",
    "    print(\"No existing Stage 1 checkpoint found. Stage 1 training will run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 Configuration (Teacher + Pretrained Assistant → Train Small Student)\n",
    "# Dual supervision with ONLY logits KD (features/attentions disabled to save memory)\n",
    "if SKIP_STAGE1:\n",
    "    print('Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.')\n",
    "else:\n",
    "    pretrained_small_ckpt = 'mitegvg/videomae-small-finetuned-ssv2-finetuned-sports-videos-in-the-wild'\n",
    "    \n",
    "    stage1_config = TriModelConfig(\n",
    "        classification_type='multiclass',\n",
    "        num_labels=num_labels,\n",
    "        teacher_model_name='mitegvg/videomae-base-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "        assistant_model_name=pretrained_small_ckpt,\n",
    "        student_model_name=pretrained_small_ckpt,\n",
    "        temperature=4.0,\n",
    "        logits_temperature=4.0,\n",
    "        teacher_logits_weight=1.0,\n",
    "        assistant_logits_weight=0.5,\n",
    "        classification_loss_weight=1.0,\n",
    "        logits_distillation_weight=0.35,\n",
    "        hidden_layers_to_align=[],\n",
    "        feature_distillation_weight=0.0,\n",
    "        attention_distillation_weight=0.0,\n",
    "        use_pretrained_student=True,\n",
    "        num_frames=16,\n",
    "        apply_defaults=False,  # NEW: prevent auto override adding hidden/attn needs\n",
    "    )\n",
    "    \n",
    "    print(\"User logits weight:\", stage1_config.logits_distillation_weight)\n",
    "    # Memory safety knobs\n",
    "    TOTAL_TRAIN_SAMPLES = 3364\n",
    "    per_device_train_batch_size = 2 if torch.cuda.is_available() else 2  # reduced\n",
    "    gradient_accumulation_steps = 16 if torch.cuda.is_available() else 8  # keep effective batch similar\n",
    "    effective_batch = per_device_train_batch_size * gradient_accumulation_steps\n",
    "    stage1_epochs = 12\n",
    "    steps_per_epoch = TOTAL_TRAIN_SAMPLES // effective_batch\n",
    "    stage1_total_steps = steps_per_epoch * stage1_epochs\n",
    "    stage1_warmup = min(500, int(0.1 * stage1_total_steps))\n",
    "    \n",
    "    print('Stage 1 config ready (logits-only KD; features/attn disabled)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Initialize Framework + Dataloaders\n",
    "if SKIP_STAGE1:\n",
    "    print('Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.')\n",
    "else:\n",
    "    setup_logging()\n",
    "\n",
    "    stage1_framework = TriModelDistillationFramework(\n",
    "        config=stage1_config,\n",
    "        num_labels=num_labels,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "    )\n",
    "\n",
    "    def _count_params(m):\n",
    "        return sum(p.numel() for p in m.parameters() if p.requires_grad), sum(p.numel() for p in m.parameters() if not p.requires_grad)\n",
    "\n",
    "    tr_s, fr_s = _count_params(stage1_framework.student_model)\n",
    "    tr_t, fr_t = _count_params(stage1_framework.teacher_model)\n",
    "    tr_a, fr_a = _count_params(stage1_framework.assistant_model)\n",
    "    print(f\"Teacher trainable {tr_t:,} frozen {fr_t:,}\")\n",
    "    print(f\"Assistant trainable {tr_a:,} frozen {fr_a:,}\")\n",
    "    print(f\"Student trainable {tr_s:,} frozen {fr_s:,}\")\n",
    "    print('Need hidden states:', getattr(stage1_framework,'_need_hidden',True), 'Need attentions:', getattr(stage1_framework,'_need_attn',True))\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        dataset_root=DATASET_ROOT,\n",
    "        image_processor=stage1_framework.image_processor,\n",
    "        label2id=label2id,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        num_frames=stage1_config.num_frames,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    print('Data loaders ready')\n",
    "\n",
    "    # Dry-run memory probe\n",
    "    if torch.cuda.is_available():\n",
    "        import gc\n",
    "        batch = next(iter(train_loader))\n",
    "        batch = {k: v.to('cuda') if hasattr(v,'to') else v for k,v in batch.items()}\n",
    "        torch.cuda.empty_cache(); gc.collect(); torch.cuda.reset_peak_memory_stats()\n",
    "        out = stage1_framework(pixel_values=batch['pixel_values'], labels=batch['labels'], output_hidden_states=False, output_attentions=False)\n",
    "        loss_probe = out['student'].logits.mean(); loss_probe.backward();\n",
    "        peak = torch.cuda.max_memory_allocated()/1024/1024\n",
    "        print(f\"Dry-run peak MB: {peak:.1f}\")\n",
    "        del out, batch, loss_probe; torch.cuda.empty_cache(); gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Stage 1 training; will use existing checkpoint as assistant in Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Train Small (Assistant-to-be)\n",
    "if SKIP_STAGE1:\n",
    "    print('Skipping Stage 1 training; will use existing checkpoint as assistant in Stage 2.')\n",
    "    stage1_trainer = None\n",
    "    stage1_framework = None\n",
    "else:\n",
    "    # Enable gradient checkpointing for student to reduce activation memory\n",
    "    if hasattr(stage1_framework.student_model, 'gradient_checkpointing_enable'):\n",
    "        stage1_framework.student_model.gradient_checkpointing_enable()\n",
    "\n",
    "    stage1_args = stage1_config.to_training_args(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=stage1_epochs,\n",
    "        warmup_steps=stage1_warmup,\n",
    "        evaluation_strategy='epoch',\n",
    "        logging_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        save_total_limit=20,\n",
    "        output_dir=STAGE1_OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=['tensorboard'],\n",
    "        logging_dir=f'{STAGE1_OUTPUT_DIR}/logs',\n",
    "    )\n",
    "\n",
    "    stage1_trainer = TriModelDistillationTrainer(\n",
    "        framework=stage1_framework,\n",
    "        distillation_config=stage1_config,\n",
    "        args=stage1_args,\n",
    "        train_dataset=train_loader.dataset,\n",
    "        eval_dataset=val_loader.dataset,\n",
    "        compute_metrics=lambda eval_pred, **kw: compute_video_classification_metrics(eval_pred, classification_type='multiclass'),\n",
    "    )\n",
    "\n",
    "    print('Starting Stage 1 training (logits-only KD, memory optimized)...')\n",
    "    stage1_train_result = stage1_trainer.train()\n",
    "    print('Stage 1 training complete')\n",
    "\n",
    "    stage1_trainer.save_model(STAGE1_OUTPUT_DIR)\n",
    "    stage1_val_metrics = stage1_trainer.evaluate(eval_dataset=val_loader.dataset)\n",
    "    print('Stage 1 validation:', stage1_val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2 Rationale\n",
    "The trained Small model now serves as a **domain-adapted bridge**. In Stage 2 we:\n",
    "- Freeze the Small checkpoint (loaded via its output directory)\n",
    "- Keep a light stabilizing signal from the original Base teacher (lower weight)\n",
    "- Emphasize logits distillation from the Assistant (higher weight)\n",
    "\n",
    "Tuning tips:\n",
    "- If Tiny underfits early: increase `assistant_logits_weight` or `logits_temperature`\n",
    "- If overfitting: reduce `classification_loss_weight` slightly or add light feature distillation\n",
    "- If training unstable: raise `teacher_logits_weight` to 0.4–0.5 for extra regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align_attention_maps: True\n",
      "align_hidden_states: False\n",
      "apply_defaults: False\n",
      "assistant_feature_weight: 1.0\n",
      "assistant_logits_weight: 1.0\n",
      "assistant_model_name: './contextual_bridge_runs_20250828_151346/stage1_teacher_to_small'\n",
      "assistant_model_path: None\n",
      "attention_distillation_weight: 0.5\n",
      "classification_loss_weight: 0.85\n",
      "classification_type: 'multiclass'\n",
      "dataset_root: 'processed_dataset'\n",
      "eval_strategy: 'epoch'\n",
      "evaluation_strategy: 'epoch'\n",
      "feature_distillation_weight: 0.0\n",
      "hidden_layers_to_align: [-1, -2]\n",
      "image_size: 224\n",
      "logging_steps: 10\n",
      "logits_distillation_weight: 0.2\n",
      "logits_temperature: 4.0\n",
      "mask_ratio: 0.0\n",
      "num_frames: 16\n",
      "num_labels: 30\n",
      "output_dir: 'tri_model_distilled_videomae'\n",
      "pretrained_student_model: 'mitegvg/videomae-tiny-finetuned-kinetics-finetuned-sports-videos-in-the-wild'\n",
      "save_strategy: 'epoch'\n",
      "save_total_limit: 3\n",
      "student_hidden_size: 384\n",
      "student_model_name: 'mitegvg/videomae-tiny-finetuned-kinetics-finetuned-sports-videos-in-the-wild'\n",
      "student_num_attention_heads: 6\n",
      "student_num_layers: 4\n",
      "teacher_feature_weight: 0.0\n",
      "teacher_logits_weight: 0.2\n",
      "teacher_model_name: 'mitegvg/videomae-base-finetuned-kinetics-finetuned-sports-videos-in-the-wild'\n",
      "temperature: 3.0\n",
      "test_csv: 'test.csv'\n",
      "train_csv: 'train.csv'\n",
      "use_pretrained_student: True\n",
      "use_tiny_student: True\n",
      "val_csv: 'val.csv'\n",
      "Stage 2 config ready (full KD: logits + features + attention)\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 Configuration (Assistant → Tiny)\n",
    "# If Stage 1 skipped, we still expect STAGE1_OUTPUT_DIR to already contain a trained small model\n",
    "if SKIP_STAGE1:\n",
    "    assert os.path.exists(os.path.join(STAGE1_OUTPUT_DIR, 'config.json')), 'Expected existing Stage 1 checkpoint missing.'\n",
    "\n",
    "# Stage 2 Configuration (Assistant → Tiny) — full logits + feature + attention KD\n",
    "stage2_config = TriModelConfig(\n",
    "    classification_type='multiclass',\n",
    "    num_labels=num_labels,\n",
    "    teacher_model_name='mitegvg/videomae-base-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "    assistant_model_name=STAGE1_OUTPUT_DIR,  # trained small (assistant)\n",
    "    student_model_name='mitegvg/videomae-tiny-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "    # Distillation temperatures\n",
    "    temperature=3.0,\n",
    "    logits_temperature=4.0,\n",
    "    # Logits KD weighting\n",
    "    teacher_logits_weight=0.2,\n",
    "    assistant_logits_weight=1.0,\n",
    "    logits_distillation_weight=0.2,\n",
    "    # Supervised classification loss\n",
    "    classification_loss_weight=0.85,\n",
    "    # Representation alignment\n",
    "    align_hidden_states=False,\n",
    "    align_attention_maps=True,\n",
    "    eval_strategy='epoch',\n",
    "    hidden_layers_to_align=[-1,-2],  # spaced to reduce redundancy & memory\n",
    "    feature_distillation_weight=0.0,\n",
    "    attention_distillation_weight=0.5,\n",
    "    # (If your loss uses these per-source feature weights)\n",
    "    teacher_feature_weight=0.0,\n",
    "    assistant_feature_weight=1.0,\n",
    "    # Student init\n",
    "    use_pretrained_student=True,\n",
    "    num_frames=16,\n",
    "    temporal_delta_distillation_weight=0.2, \n",
    "    temporal_delta_layers=[-1]\n",
    "    apply_defaults=False,\n",
    ")\n",
    "for k in sorted(stage2_config.__dataclass_fields__.keys()):\n",
    "    print(f\"{k}: {getattr(stage2_config, k)!r}\")\n",
    "stage2_epochs = 20\n",
    "stage2_total_steps = steps_per_epoch * stage2_epochs\n",
    "stage2_warmup = min(500, int(0.1 * stage2_total_steps))\n",
    "print('Stage 2 config ready (full KD: logits + features + attention)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cache cleared before Stage 2 initialization.\n",
      "Starting Stage 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1025' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1025/2100 2:47:17 < 2:55:47, 0.10 it/s, Epoch 9.67/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 59\u001b[0m\n\u001b[0;32m     49\u001b[0m stage2_trainer \u001b[38;5;241m=\u001b[39m TriModelDistillationTrainer(\n\u001b[0;32m     50\u001b[0m     framework\u001b[38;5;241m=\u001b[39mstage2_framework,\n\u001b[0;32m     51\u001b[0m     distillation_config\u001b[38;5;241m=\u001b[39mstage2_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m eval_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: compute_video_classification_metrics(eval_pred, classification_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting Stage 2 training...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m stage2_train_result \u001b[38;5;241m=\u001b[39m \u001b[43mstage2_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStage 2 training complete\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m stage2_trainer\u001b[38;5;241m.\u001b[39msave_model(STAGE2_OUTPUT_DIR)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\trainer.py:2514\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2512\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2513\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2514\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2516\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\transformers\\trainer.py:5243\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[0;32m   5241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5243\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   5244\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5245\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\accelerate\\data_loader.py:462\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 462\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\videomae\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\BIRKBECK\\REPOS\\videomae-base-finetuned-sports-in-the-wild\\tri_model_distillation\\utils.py:156\u001b[0m, in \u001b[0;36mVideoMAEDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    152\u001b[0m video_path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_paths_and_labels[idx]\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Load video frames\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_video_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# Process frames\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(\n\u001b[0;32m    160\u001b[0m         frames, \n\u001b[0;32m    161\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m    162\u001b[0m         do_rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Frames are already normalized\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     )\n",
      "File \u001b[1;32mD:\\BIRKBECK\\REPOS\\videomae-base-finetuned-sports-in-the-wild\\tri_model_distillation\\utils.py:193\u001b[0m, in \u001b[0;36mVideoMAEDataset._load_video_frames\u001b[1;34m(self, video_path, num_frames)\u001b[0m\n\u001b[0;32m    190\u001b[0m frame_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, total_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, num_frames, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_idx \u001b[38;5;129;01min\u001b[39;00m frame_indices:\n\u001b[1;32m--> 193\u001b[0m     \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# Use last successful frame if read fails\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Stage 2: Initialize + Train Tiny with Frozen Assistant\n",
    "# Clean up Stage 1 objects to free GPU memory before constructing Stage 2 framework\n",
    "if torch.cuda.is_available():\n",
    "    import gc\n",
    "    if not SKIP_STAGE1:\n",
    "        del stage1_trainer, stage1_framework\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    print('CUDA memory cache cleared before Stage 2 initialization.')\n",
    "\n",
    "stage2_framework = TriModelDistillationFramework(\n",
    "    config=stage2_config,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "train_loader2, val_loader2, test_loader2 = create_data_loaders(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    image_processor=stage2_framework.image_processor,\n",
    "    label2id=label2id,\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    num_frames=stage2_config.num_frames,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "stage2_args = stage2_config.to_training_args(\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=stage2_epochs,\n",
    "    warmup_steps=stage2_warmup,\n",
    "    eval_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=20,\n",
    "    output_dir=STAGE2_OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=['tensorboard', 'wandb'],\n",
    "    logging_dir=f'{STAGE2_OUTPUT_DIR}/logs',\n",
    ")\n",
    "\n",
    "stage2_trainer = TriModelDistillationTrainer(\n",
    "    framework=stage2_framework,\n",
    "    distillation_config=stage2_config,\n",
    "    args=stage2_args,\n",
    "    train_dataset=train_loader2.dataset,\n",
    "    eval_dataset=val_loader2.dataset,\n",
    "    compute_metrics=lambda eval_pred, **kw: compute_video_classification_metrics(eval_pred, classification_type='multiclass'),\n",
    ")\n",
    "\n",
    "print('Starting Stage 2 training...')\n",
    "stage2_train_result = stage2_trainer.train()\n",
    "print('Stage 2 training complete')\n",
    "\n",
    "stage2_trainer.save_model(STAGE2_OUTPUT_DIR)\n",
    "stage2_val_metrics = stage2_trainer.evaluate(eval_dataset=val_loader2.dataset)\n",
    "print('Stage 2 validation:', stage2_val_metrics)\n",
    "\n",
    "# For downstream evaluation cells\n",
    "OUTPUT_DIR = STAGE2_OUTPUT_DIR\n",
    "framework = stage2_framework\n",
    "val_loader = val_loader2\n",
    "test_loader = test_loader2\n",
    "print('OUTPUT_DIR set to final student:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on the full test set...\n",
      "./contextual_bridge_runs_20250828_064753/stage2_small_to_tiny\n",
      "Loaded 422 samples from processed_dataset\\test.csv\n",
      "\n",
      "Starting inference on 422 test videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|████████████████████████████| 422/422 [01:21<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Total videos in test set: 422\n",
      "Videos successfully processed: 422\n",
      "Videos skipped (missing/corrupt): 0\n",
      "Top-1 Correct Predictions: 201\n",
      "Top-5 Correct Predictions: 346\n",
      "Top-1 Accuracy: 47.63%\n",
      "Top-5 Accuracy: 81.99%\n",
      "Average inference time per video: 0.011 seconds (89.01 videos/sec)\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     archery       0.46      0.46      0.46        13\n",
      "    baseball       0.67      0.33      0.44        18\n",
      "  basketball       0.50      0.25      0.33        12\n",
      "         bmx       0.50      0.18      0.27        11\n",
      "     bowling       0.44      0.70      0.54        10\n",
      "      boxing       0.00      0.00      0.00        11\n",
      "cheerleading       0.70      0.37      0.48        19\n",
      " discusthrow       0.22      0.50      0.31         4\n",
      "      diving       0.70      0.64      0.67        11\n",
      "    football       0.50      0.65      0.57        20\n",
      "        golf       0.50      0.55      0.52        11\n",
      "  gymnastics       0.35      0.71      0.47        17\n",
      " hammerthrow       0.55      0.38      0.44        16\n",
      "    highjump       0.40      0.25      0.31        16\n",
      "      hockey       0.56      0.71      0.62        14\n",
      "    hurdling       0.33      0.14      0.20        14\n",
      "     javelin       0.33      0.60      0.43        10\n",
      "    longjump       0.21      0.23      0.22        13\n",
      "   polevault       0.20      0.43      0.27         7\n",
      "      rowing       0.62      0.47      0.53        17\n",
      "     running       0.00      0.00      0.00        11\n",
      "     shotput       0.75      0.18      0.29        17\n",
      "     skating       0.67      0.62      0.64        13\n",
      "      skiing       0.71      0.62      0.67        16\n",
      "      soccer       0.48      0.77      0.59        13\n",
      "    swimming       0.78      0.97      0.86        29\n",
      "      tennis       0.40      0.60      0.48        10\n",
      "  volleyball       0.41      0.50      0.45        24\n",
      "      weight       0.27      0.43      0.33         7\n",
      "   wrestling       0.44      0.44      0.44        18\n",
      "\n",
      "    accuracy                           0.48       422\n",
      "   macro avg       0.46      0.46      0.43       422\n",
      "weighted avg       0.49      0.48      0.46       422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace the problematic last cell with this corrected version:\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Starting evaluation on the full test set...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load model directly (not using pipeline)\n",
    "local_model_directory = OUTPUT_DIR\n",
    "print(OUTPUT_DIR)\n",
    "student_model = VideoMAEForVideoClassification.from_pretrained(local_model_directory)\n",
    "processor = VideoMAEImageProcessor.from_pretrained(local_model_directory)\n",
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "def process_video_for_inference(video_path, processor, num_frames=16):\n",
    "    \"\"\"Process video exactly like the training pipeline\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "        \n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if frame_count < num_frames:\n",
    "            cap.release()\n",
    "            return None\n",
    "            \n",
    "        # Sample frames uniformly (same as training)\n",
    "        frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) != num_frames:\n",
    "            return None\n",
    "            \n",
    "        # Process frames using the same processor\n",
    "        inputs = processor(frames, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load test data from CSV\n",
    "def load_test_data_from_csv(csv_file_path, data_root_path):\n",
    "    test_samples = []\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"ERROR: Test CSV file not found at {csv_file_path}\")\n",
    "        return test_samples\n",
    "\n",
    "    with open(csv_file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f.readlines(), 1):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                relative_video_path = parts[0]\n",
    "                true_label_str = parts[1]\n",
    "                full_video_path = os.path.normpath(os.path.join(data_root_path, relative_video_path))\n",
    "                test_samples.append((full_video_path, true_label_str))\n",
    "            elif line.strip():\n",
    "                print(f\"Warning: Malformed line {line_num} in {csv_file_path}: '{line.strip()}'\")\n",
    "                \n",
    "    print(f\"Loaded {len(test_samples)} samples from {csv_file_path}\")\n",
    "    return test_samples\n",
    "\n",
    "# Load test data\n",
    "dataset_root_path = \"processed_dataset\"\n",
    "test_csv_path = os.path.join(dataset_root_path, \"test.csv\")\n",
    "test_data = load_test_data_from_csv(test_csv_path, dataset_root_path)\n",
    "\n",
    "if test_data:\n",
    "    total_videos_processed = 0\n",
    "    videos_skipped = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    top1_correct_predictions = 0\n",
    "    top5_correct_predictions = 0\n",
    "    inference_times = []\n",
    "\n",
    "    print(f\"\\nStarting inference on {len(test_data)} test videos...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (video_path, true_label) in enumerate(tqdm(test_data, desc=\"Processing videos\")):\n",
    "            if not os.path.exists(video_path):\n",
    "                videos_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Process video using the same pipeline as training\n",
    "            inputs = process_video_for_inference(video_path, processor)\n",
    "            if inputs is None:\n",
    "                videos_skipped += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Move inputs to device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Get model predictions\n",
    "                outputs = student_model(**inputs)\n",
    "                logits = outputs.logits[0]  # Remove batch dimension\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                \n",
    "                # Get top-5 predictions\n",
    "                top5_probs, top5_indices = torch.topk(probs, 5)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                inference_times.append(end_time - start_time)\n",
    "                total_videos_processed += 1\n",
    "\n",
    "                # Convert indices to labels using id2label\n",
    "                predicted_labels_top5 = [id2label[idx.item()] for idx in top5_indices]\n",
    "                predicted_label_top1 = predicted_labels_top5[0]\n",
    "\n",
    "                # Store for metrics calculation\n",
    "                predicted_labels.append(predicted_label_top1)\n",
    "                true_labels.append(true_label)\n",
    "\n",
    "                # Calculate top-k accuracy\n",
    "                if predicted_label_top1 == true_label:\n",
    "                    top1_correct_predictions += 1\n",
    "                if true_label in predicted_labels_top5:\n",
    "                    top5_correct_predictions += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during inference for {video_path}: {e}\")\n",
    "                videos_skipped += 1\n",
    "\n",
    "    # Print results\n",
    "    if total_videos_processed > 0:\n",
    "        top1_accuracy = (top1_correct_predictions / total_videos_processed) * 100\n",
    "        top5_accuracy = (top5_correct_predictions / total_videos_processed) * 100\n",
    "        avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "        fps = 1.0 / avg_inference_time if avg_inference_time > 0 else float('inf')\n",
    "\n",
    "        print(\"\\n--- Evaluation Complete ---\")\n",
    "        print(f\"Total videos in test set: {len(test_data)}\")\n",
    "        print(f\"Videos successfully processed: {total_videos_processed}\")\n",
    "        print(f\"Videos skipped (missing/corrupt): {videos_skipped}\")\n",
    "        print(f\"Top-1 Correct Predictions: {top1_correct_predictions}\")\n",
    "        print(f\"Top-5 Correct Predictions: {top5_correct_predictions}\")\n",
    "        print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
    "        print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
    "        print(f\"Average inference time per video: {avg_inference_time:.3f} seconds ({fps:.2f} videos/sec)\")\n",
    "\n",
    "        # Classification report\n",
    "        if len(predicted_labels) == len(true_labels) and len(true_labels) > 0:\n",
    "            print(\"\\nDetailed Classification Report:\")\n",
    "            print(classification_report(true_labels, predicted_labels, labels=list(label2id.keys()), zero_division=0))\n",
    "    else:\n",
    "        print(\"No videos were processed successfully.\")\n",
    "else:\n",
    "    print(\"No test data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

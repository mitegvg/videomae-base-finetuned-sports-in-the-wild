{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Two-Stage Contextual Bridge Distillation — Sports-in-the-Wild\n",
    "\n",
    "This notebook now implements a **sequential 2-stage knowledge bridge** instead of training all three models jointly. The idea:\n",
    "\n",
    "```\n",
    "Teacher (Base)  ──►  Stage 1: Train Small  ──►  Stage 2: Small (frozen) teaches Tiny\n",
    "        (global rich knowledge)              (domain-adapted bridge)         (final efficient student)\n",
    "```\n",
    "\n",
    "Why a contextual bridge?\n",
    "- The direct gap Teacher(Base) → Student(Tiny) can be large (capacity + representation mismatch).\n",
    "- First adapting an intermediate (Small) yields a *domain-specialized assistant*.\n",
    "- Then the Tiny model learns from both: retained high-level signal (Teacher) + distilled, compressed domain signal (Assistant).\n",
    "\n",
    "Outcome: Better stability + improved tiny accuracy vs single-hop distillation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Flow Overview\n",
    "\n",
    "We run **two independent training jobs** inside one notebook:\n",
    "\n",
    "### Stage 1: Teacher → Assistant (Train ViT-Small)\n",
    "Goal: Produce a strong, domain-adapted SMALL model that will act as a frozen assistant in Stage 2.\n",
    "\n",
    "Configuration principles:\n",
    "- Student model = `videomae-small` (becomes the assistant later)\n",
    "- Assistant influence weights = 0 (no assistant yet)\n",
    "- Only Teacher → Student logits (and optionally features) KD\n",
    "\n",
    "### Stage 2: Assistant → Student (Train ViT-Tiny)\n",
    "Goal: Train the TINY model using BOTH the frozen Small (assistant) and the original Teacher.\n",
    "\n",
    "Configuration principles:\n",
    "- Assistant model path = checkpoint directory produced in Stage 1\n",
    "- Teacher still provides a small stabilizing signal\n",
    "- Assistant has higher logits weight (primary mentor)\n",
    "\n",
    "### Advantages of This Design\n",
    "- Reduces representational jump distance\n",
    "- Lets the Assistant internalize domain specifics before mentoring Tiny\n",
    "- Often yields +accuracy versus a direct Teacher→Tiny pipeline\n",
    "\n",
    "Proceed through sections in order. Skip Stage 1 only if you already have a trained Small checkpoint you want to reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token successfully loaded from HUGGINGFACE_TOKEN environment variable.\n",
      "2.1.0+cu118\n",
      "CUDA available: True\n",
      "torch: 2.1.0+cu118 cuda: 11.8\n",
      "torchvision: 0.16.0+cu118\n",
      "pytorchvideo: 0.1.5\n",
      "has functional_tensor: True\n"
     ]
    }
   ],
   "source": [
    "# Environment / Common Imports\n",
    "import os, json, torch\n",
    "import torchvision, pytorchvideo, transformers\n",
    "from huggingface_hub import HfFolder\n",
    "from datetime import datetime\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from tri_model_distillation.config import TriModelConfig\n",
    "from tri_model_distillation.models import TriModelDistillationFramework\n",
    "from tri_model_distillation.trainer import TriModelDistillationTrainer, compute_video_classification_metrics\n",
    "from tri_model_distillation.utils import (\n",
    "    setup_logging, load_label_mappings, create_data_loaders,\n",
    ")\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if token:\n",
    "    HfFolder.save_token(token)\n",
    "    print(\"Hugging Face token successfully loaded from HUGGINGFACE_TOKEN environment variable.\")\n",
    "else:\n",
    "    print(\"HUGGINGFACE_TOKEN environment variable not set. If you want to push models to the Hub, please set this variable before starting Jupyter Lab.\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"pytorchvideo:\", pytorchvideo.__version__)\n",
    "print(\"has functional_tensor:\", hasattr(__import__('torchvision.transforms', fromlist=['']), 'functional_tensor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./contextual_bridge_runs_20250830_111437\n",
      "Detected 30 classes\n",
      "['archery', 'baseball', 'basketball', 'bmx', 'bowling', 'boxing', 'cheerleading', 'discusthrow', 'diving', 'football', 'golf', 'gymnastics', 'hammerthrow', 'highjump', 'hockey', 'hurdling', 'javelin', 'longjump', 'polevault', 'rowing', 'running', 'shotput', 'skating', 'skiing', 'soccer', 'swimming', 'tennis', 'volleyball', 'weight', 'wrestling'] ...\n"
     ]
    }
   ],
   "source": [
    "# Dataset + Label Mapping\n",
    "DATASET_ROOT = 'processed_dataset'\n",
    "BASE_RUN_DIR = f\"./contextual_bridge_runs_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "BASE_RUN_DIR = f\"./contextual_bridge_runs_20250830_111437\"\n",
    "\n",
    "STAGE1_OUTPUT_DIR = f\"{BASE_RUN_DIR}/stage1_teacher_to_small\"\n",
    "STAGE2_OUTPUT_DIR = f\"{BASE_RUN_DIR}/stage2_small_to_tiny\"\n",
    "os.makedirs(STAGE1_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(STAGE2_OUTPUT_DIR, exist_ok=True)\n",
    "print(BASE_RUN_DIR);\n",
    "\n",
    "label2id, id2label = load_label_mappings(dataset_root=DATASET_ROOT, train_csv='train.csv', classification_type='multiclass')\n",
    "num_labels = len(label2id)\n",
    "print(f\"Detected {num_labels} classes\")\n",
    "print(list(label2id.keys())[:30], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Stage 1 checkpoint detected in ./contextual_bridge_runs_20250830_111437/stage1_teacher_to_small. Stage 1 training will be skipped.\n",
      "(Delete or rename the directory to force retraining.)\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 Skip Logic — Detect existing checkpoint to optionally skip Stage 1 training\n",
    "import os, glob\n",
    "stage1_checkpoint_exists = any(\n",
    "    os.path.exists(os.path.join(STAGE1_OUTPUT_DIR, fname))\n",
    "    for fname in ['pytorch_model.bin', 'model.safetensors', 'config.json']\n",
    ")\n",
    "if not stage1_checkpoint_exists:\n",
    "    for sub in glob.glob(os.path.join(STAGE1_OUTPUT_DIR, 'checkpoint-*')):\n",
    "        if os.path.exists(os.path.join(sub, 'pytorch_model.bin')) or os.path.exists(os.path.join(sub, 'model.safetensors')):\n",
    "            stage1_checkpoint_exists = True\n",
    "            break\n",
    "\n",
    "SKIP_STAGE1 = stage1_checkpoint_exists\n",
    "if SKIP_STAGE1:\n",
    "    print(f\"Existing Stage 1 checkpoint detected in {STAGE1_OUTPUT_DIR}. Stage 1 training will be skipped.\")\n",
    "    print(\"(Delete or rename the directory to force retraining.)\")\n",
    "else:\n",
    "    print(\"No existing Stage 1 checkpoint found. Stage 1 training will run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 Configuration (Teacher + Pretrained Assistant → Train Small Student)\n",
    "# Dual supervision with ONLY logits KD (features/attentions disabled to save memory)\n",
    "if SKIP_STAGE1:\n",
    "    print('Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.')\n",
    "else:\n",
    "    pretrained_small_ckpt = 'mitegvg/videomae-small-finetuned-ssv2-finetuned-sports-videos-in-the-wild'\n",
    "    \n",
    "    stage1_config = TriModelConfig(\n",
    "        classification_type='multiclass',\n",
    "        num_labels=num_labels,\n",
    "        teacher_model_name='mitegvg/videomae-base-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "        assistant_model_name=pretrained_small_ckpt,\n",
    "        student_model_name=pretrained_small_ckpt,\n",
    "        temperature=4.0,\n",
    "        logits_temperature=4.0,\n",
    "        teacher_logits_weight=1.0,\n",
    "        assistant_logits_weight=0.5,\n",
    "        classification_loss_weight=1.0,\n",
    "        logits_distillation_weight=0.35,\n",
    "        hidden_layers_to_align=[],\n",
    "        feature_distillation_weight=0.0,\n",
    "        attention_distillation_weight=0.0,\n",
    "        use_pretrained_student=True,\n",
    "        num_frames=16,\n",
    "        apply_defaults=False,  # NEW: prevent auto override adding hidden/attn needs\n",
    "    )\n",
    "    \n",
    "    print(\"User logits weight:\", stage1_config.logits_distillation_weight)\n",
    "    # Memory safety knobs\n",
    "    TOTAL_TRAIN_SAMPLES = 3364\n",
    "    per_device_train_batch_size = 2 if torch.cuda.is_available() else 2  # reduced\n",
    "    gradient_accumulation_steps = 16 if torch.cuda.is_available() else 8  # keep effective batch similar\n",
    "    effective_batch = per_device_train_batch_size * gradient_accumulation_steps\n",
    "    stage1_epochs = 12\n",
    "    steps_per_epoch = TOTAL_TRAIN_SAMPLES // effective_batch\n",
    "    stage1_total_steps = steps_per_epoch * stage1_epochs\n",
    "    stage1_warmup = min(500, int(0.1 * stage1_total_steps))\n",
    "    \n",
    "    print('Stage 1 config ready (logits-only KD; features/attn disabled)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Initialize Framework + Dataloaders\n",
    "if SKIP_STAGE1:\n",
    "    print('Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.')\n",
    "else:\n",
    "    setup_logging()\n",
    "\n",
    "    stage1_framework = TriModelDistillationFramework(\n",
    "        config=stage1_config,\n",
    "        num_labels=num_labels,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "    )\n",
    "\n",
    "    def _count_params(m):\n",
    "        return sum(p.numel() for p in m.parameters() if p.requires_grad), sum(p.numel() for p in m.parameters() if not p.requires_grad)\n",
    "\n",
    "    tr_s, fr_s = _count_params(stage1_framework.student_model)\n",
    "    tr_t, fr_t = _count_params(stage1_framework.teacher_model)\n",
    "    tr_a, fr_a = _count_params(stage1_framework.assistant_model)\n",
    "    print(f\"Teacher trainable {tr_t:,} frozen {fr_t:,}\")\n",
    "    print(f\"Assistant trainable {tr_a:,} frozen {fr_a:,}\")\n",
    "    print(f\"Student trainable {tr_s:,} frozen {fr_s:,}\")\n",
    "    print('Need hidden states:', getattr(stage1_framework,'_need_hidden',True), 'Need attentions:', getattr(stage1_framework,'_need_attn',True))\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        dataset_root=DATASET_ROOT,\n",
    "        image_processor=stage1_framework.image_processor,\n",
    "        label2id=label2id,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        num_frames=stage1_config.num_frames,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    print('Data loaders ready')\n",
    "\n",
    "    # Dry-run memory probe\n",
    "    if torch.cuda.is_available():\n",
    "        import gc\n",
    "        batch = next(iter(train_loader))\n",
    "        batch = {k: v.to('cuda') if hasattr(v,'to') else v for k,v in batch.items()}\n",
    "        torch.cuda.empty_cache(); gc.collect(); torch.cuda.reset_peak_memory_stats()\n",
    "        out = stage1_framework(pixel_values=batch['pixel_values'], labels=batch['labels'], output_hidden_states=False, output_attentions=False)\n",
    "        loss_probe = out['student'].logits.mean(); loss_probe.backward();\n",
    "        peak = torch.cuda.max_memory_allocated()/1024/1024\n",
    "        print(f\"Dry-run peak MB: {peak:.1f}\")\n",
    "        del out, batch, loss_probe; torch.cuda.empty_cache(); gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Stage 1 training; will use existing checkpoint as assistant in Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Train Small (Assistant-to-be)\n",
    "if SKIP_STAGE1:\n",
    "    print('Skipping Stage 1 training; will use existing checkpoint as assistant in Stage 2.')\n",
    "    stage1_trainer = None\n",
    "    stage1_framework = None\n",
    "else:\n",
    "    # Enable gradient checkpointing for student to reduce activation memory\n",
    "    if hasattr(stage1_framework.student_model, 'gradient_checkpointing_enable'):\n",
    "        stage1_framework.student_model.gradient_checkpointing_enable()\n",
    "\n",
    "    stage1_args = stage1_config.to_training_args(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=stage1_epochs,\n",
    "        warmup_steps=stage1_warmup,\n",
    "        evaluation_strategy='epoch',\n",
    "        logging_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        save_total_limit=20,\n",
    "        output_dir=STAGE1_OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=['tensorboard'],\n",
    "        logging_dir=f'{STAGE1_OUTPUT_DIR}/logs',\n",
    "    )\n",
    "\n",
    "    stage1_trainer = TriModelDistillationTrainer(\n",
    "        framework=stage1_framework,\n",
    "        distillation_config=stage1_config,\n",
    "        args=stage1_args,\n",
    "        train_dataset=train_loader.dataset,\n",
    "        eval_dataset=val_loader.dataset,\n",
    "        compute_metrics=lambda eval_pred, **kw: compute_video_classification_metrics(eval_pred, classification_type='multiclass'),\n",
    "    )\n",
    "\n",
    "    print('Starting Stage 1 training (logits-only KD, memory optimized)...')\n",
    "    stage1_train_result = stage1_trainer.train()\n",
    "    print('Stage 1 training complete')\n",
    "\n",
    "    stage1_trainer.save_model(STAGE1_OUTPUT_DIR)\n",
    "    stage1_val_metrics = stage1_trainer.evaluate(eval_dataset=val_loader.dataset)\n",
    "    print('Stage 1 validation:', stage1_val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2 Rationale\n",
    "The trained Small model now serves as a **domain-adapted bridge**. In Stage 2 we:\n",
    "- Freeze the Small checkpoint (loaded via its output directory)\n",
    "- Keep a light stabilizing signal from the original Base teacher (lower weight)\n",
    "- Emphasize logits distillation from the Assistant (higher weight)\n",
    "\n",
    "Tuning tips:\n",
    "- If Tiny underfits early: increase `assistant_logits_weight` or `logits_temperature`\n",
    "- If overfitting: reduce `classification_loss_weight` slightly or add light feature distillation\n",
    "- If training unstable: raise `teacher_logits_weight` to 0.4–0.5 for extra regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align_attention_maps: True\n",
      "align_hidden_states: True\n",
      "apply_defaults: False\n",
      "assistant_attention_weight: 0.7\n",
      "assistant_feature_weight: 0.7\n",
      "assistant_logits_weight: 0.8\n",
      "assistant_model_name: './contextual_bridge_runs_20250830_111437/stage1_teacher_to_small'\n",
      "assistant_model_path: None\n",
      "attention_distillation_weight: 0.3\n",
      "attention_head_squeeze: True\n",
      "attention_head_squeeze_mode: 'learned'\n",
      "classification_loss_weight: 0.5\n",
      "classification_type: 'multiclass'\n",
      "dataset_root: 'processed_dataset'\n",
      "enable_layer_fusion: False\n",
      "eval_strategy: None\n",
      "evaluation_strategy: 'epoch'\n",
      "feature_distillation_weight: 0.2\n",
      "fusion_assistant_weight: 0.5\n",
      "fusion_projection_dim: None\n",
      "fusion_source: 'teacher'\n",
      "fusion_source_weighting: 'learned'\n",
      "fusion_teacher_weight: 0.5\n",
      "head_squeeze_ortho_weight: 0.001\n",
      "hidden_layers_to_align: [-8, -4, -1]\n",
      "image_size: 224\n",
      "kd_conf_threshold: 0.5\n",
      "kd_dynamic_lower: True\n",
      "kd_min_keep_ratio: 0.25\n",
      "layer_fusion_assistant_layers: []\n",
      "layer_fusion_mode: 'attention'\n",
      "layer_fusion_teacher_layers: []\n",
      "logging_steps: 10\n",
      "logits_distillation_weight: 0.2\n",
      "logits_temperature: 4.0\n",
      "mask_ratio: 0.0\n",
      "num_frames: 16\n",
      "num_labels: 30\n",
      "output_dir: 'tri_model_distilled_videomae'\n",
      "pretrained_student_model: 'mitegvg/videomae-tiny-12-finetuned-kinetics-finetuned-sports-videos-in-the-wild'\n",
      "require_attentions: True\n",
      "require_hidden_states: True\n",
      "save_strategy: 'epoch'\n",
      "save_total_limit: 3\n",
      "student_hidden_size: 384\n",
      "student_model_name: 'mitegvg/videomae-tiny-12-finetuned-kinetics-finetuned-sports-videos-in-the-wild'\n",
      "student_num_attention_heads: 6\n",
      "student_num_layers: 4\n",
      "teacher_attention_weight: 0.1\n",
      "teacher_feature_weight: 0.1\n",
      "teacher_logits_weight: 0.1\n",
      "teacher_model_name: 'mitegvg/videomae-base-finetuned-kinetics-finetuned-sports-videos-in-the-wild'\n",
      "temperature: 4.0\n",
      "temporal_delta_distillation_weight: 0.05\n",
      "temporal_delta_layers: [-4, -1]\n",
      "temporal_delta_use_projection: False\n",
      "test_csv: 'test.csv'\n",
      "train_csv: 'train.csv'\n",
      "use_pretrained_student: True\n",
      "use_tiny_student: False\n",
      "val_csv: 'val.csv'\n",
      "Stage2: steps/epoch=106, total_steps=2120, warmup=100\n",
      "Stage 2 config ready (full KD: logits + features + attention)\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 Configuration (Assistant → Tiny)\n",
    "# If Stage 1 skipped, we still expect STAGE1_OUTPUT_DIR to already contain a trained small model\n",
    "import math\n",
    "if SKIP_STAGE1:\n",
    "    assert os.path.exists(os.path.join(STAGE1_OUTPUT_DIR, 'config.json')), 'Expected existing Stage 1 checkpoint missing.'\n",
    "\n",
    "# Stage 2 Configuration (Assistant → Tiny) — full logits + feature + attention KD\n",
    "stage2_config = TriModelConfig(\n",
    "    classification_type='multiclass',\n",
    "    num_labels=num_labels,\n",
    "    teacher_model_name='mitegvg/videomae-base-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "    assistant_model_name=STAGE1_OUTPUT_DIR,\n",
    "    student_model_name='mitegvg/videomae-tiny-12-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "    use_pretrained_student=True,\n",
    "    use_tiny_student=False,\n",
    "\n",
    " # --- Logits KD (now uses both teacher & assistant) ---\n",
    "    temperature=4.0,\n",
    "    logits_temperature=4.0,\n",
    "    teacher_logits_weight=0.1,\n",
    "    assistant_logits_weight=1.0,\n",
    "    logits_distillation_weight=0.20,  \n",
    "\n",
    "    # --- Feature & Attention KD (turn features on; keep attention moderate) ---\n",
    "    feature_distillation_weight=0.08,  \n",
    "    attention_distillation_weight=0.2,\n",
    "    hidden_layers_to_align=[-8, -4, -1],  # mid + late\n",
    "    teacher_feature_weight=0.1,\n",
    "    assistant_feature_weight=0.7,\n",
    "    teacher_attention_weight=0.1,\n",
    "    assistant_attention_weight=0.7,\n",
    "\n",
    "    # Head squeeze stays on (assistant often has more heads than tiny)\n",
    "    attention_head_squeeze=False,\n",
    "    attention_head_squeeze_mode=\"learned\",\n",
    "    head_squeeze_ortho_weight=1e-3,\n",
    "\n",
    "    # --- CE weight slightly reduced to let KD act ---\n",
    "    classification_loss_weight=0.85, \n",
    "\n",
    "    # Runtime\n",
    "    num_frames=16,\n",
    "    require_hidden_states=True,\n",
    "    require_attentions=True,\n",
    "\n",
    "    apply_defaults=False,\n",
    ")\n",
    "for k in sorted(stage2_config.__dataclass_fields__.keys()):\n",
    "    print(f\"{k}: {getattr(stage2_config, k)!r}\")\n",
    "stage2_epochs = 20\n",
    "TOTAL_TRAIN_SAMPLES = 3364\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 16 if torch.cuda.is_available() else 8\n",
    "effective_batch = per_device_train_batch_size * gradient_accumulation_steps  # 32 if CUDA\n",
    "steps_per_epoch = math.ceil(TOTAL_TRAIN_SAMPLES / effective_batch)          # ≈105, not 3364\n",
    "stage2_total_steps = steps_per_epoch * stage2_epochs                        # ≈2100\n",
    "stage2_warmup = min(100, int(0.05 * stage2_total_steps))                    # shorter warmup (≤100)\n",
    "print(f\"Stage2: steps/epoch={steps_per_epoch}, total_steps={stage2_total_steps}, warmup={stage2_warmup}\")\n",
    "print('Stage 2 config ready (full KD: logits + features + attention)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cache cleared before Stage 2 initialization.\n",
      "Starting Stage 2 training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mite_gvg (mitegvg) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\BIRKBECK\\REPOS\\videomae-base-finetuned-sports-in-the-wild\\wandb\\run-20250831_201039-im66o86r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitegvg/huggingface/runs/im66o86r' target=\"_blank\">./contextual_bridge_runs_20250830_111437/stage2_small_to_tiny</a></strong> to <a href='https://wandb.ai/mitegvg/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitegvg/huggingface' target=\"_blank\">https://wandb.ai/mitegvg/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitegvg/huggingface/runs/im66o86r' target=\"_blank\">https://wandb.ai/mitegvg/huggingface/runs/im66o86r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2100/2100 6:11:23, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Classification Loss</th>\n",
       "      <th>Feature Distillation Loss</th>\n",
       "      <th>Attention Distillation Loss</th>\n",
       "      <th>Logits Distillation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Num Classes</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted Precision</th>\n",
       "      <th>Weighted Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.814000</td>\n",
       "      <td>4.017619</td>\n",
       "      <td>1.998621</td>\n",
       "      <td>0.937931</td>\n",
       "      <td>0.278074</td>\n",
       "      <td>1.251832</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.406210</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>0.387558</td>\n",
       "      <td>30</td>\n",
       "      <td>0.406210</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>0.387558</td>\n",
       "      <td>0.411950</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.396819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.015300</td>\n",
       "      <td>2.878715</td>\n",
       "      <td>2.015356</td>\n",
       "      <td>0.861008</td>\n",
       "      <td>0.229731</td>\n",
       "      <td>1.236423</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.416926</td>\n",
       "      <td>0.402707</td>\n",
       "      <td>0.395085</td>\n",
       "      <td>30</td>\n",
       "      <td>0.416926</td>\n",
       "      <td>0.402707</td>\n",
       "      <td>0.395085</td>\n",
       "      <td>0.431900</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.410910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.364000</td>\n",
       "      <td>2.516181</td>\n",
       "      <td>1.976314</td>\n",
       "      <td>0.802652</td>\n",
       "      <td>0.208752</td>\n",
       "      <td>1.186929</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.456458</td>\n",
       "      <td>0.444355</td>\n",
       "      <td>0.431245</td>\n",
       "      <td>30</td>\n",
       "      <td>0.456458</td>\n",
       "      <td>0.444355</td>\n",
       "      <td>0.431245</td>\n",
       "      <td>0.475440</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.448030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.057100</td>\n",
       "      <td>2.296709</td>\n",
       "      <td>1.924893</td>\n",
       "      <td>0.757186</td>\n",
       "      <td>0.195354</td>\n",
       "      <td>1.144740</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.467353</td>\n",
       "      <td>0.435669</td>\n",
       "      <td>0.435131</td>\n",
       "      <td>30</td>\n",
       "      <td>0.467353</td>\n",
       "      <td>0.435669</td>\n",
       "      <td>0.435131</td>\n",
       "      <td>0.485950</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.454618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.850300</td>\n",
       "      <td>2.162686</td>\n",
       "      <td>1.924168</td>\n",
       "      <td>0.722618</td>\n",
       "      <td>0.188384</td>\n",
       "      <td>1.099456</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.431536</td>\n",
       "      <td>0.415233</td>\n",
       "      <td>0.409493</td>\n",
       "      <td>30</td>\n",
       "      <td>0.431536</td>\n",
       "      <td>0.415233</td>\n",
       "      <td>0.409493</td>\n",
       "      <td>0.446457</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.429150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.685900</td>\n",
       "      <td>2.063668</td>\n",
       "      <td>1.904437</td>\n",
       "      <td>0.693759</td>\n",
       "      <td>0.186264</td>\n",
       "      <td>1.091537</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.449113</td>\n",
       "      <td>0.436297</td>\n",
       "      <td>0.432694</td>\n",
       "      <td>30</td>\n",
       "      <td>0.449113</td>\n",
       "      <td>0.436297</td>\n",
       "      <td>0.432694</td>\n",
       "      <td>0.467698</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.452856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.553400</td>\n",
       "      <td>1.989214</td>\n",
       "      <td>1.908132</td>\n",
       "      <td>0.668443</td>\n",
       "      <td>0.182252</td>\n",
       "      <td>1.056333</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.456481</td>\n",
       "      <td>0.445193</td>\n",
       "      <td>0.439545</td>\n",
       "      <td>30</td>\n",
       "      <td>0.456481</td>\n",
       "      <td>0.445193</td>\n",
       "      <td>0.439545</td>\n",
       "      <td>0.470602</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.455459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.447700</td>\n",
       "      <td>1.937195</td>\n",
       "      <td>1.919598</td>\n",
       "      <td>0.646962</td>\n",
       "      <td>0.180719</td>\n",
       "      <td>1.040314</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.484457</td>\n",
       "      <td>0.463079</td>\n",
       "      <td>0.462165</td>\n",
       "      <td>30</td>\n",
       "      <td>0.484457</td>\n",
       "      <td>0.463079</td>\n",
       "      <td>0.462165</td>\n",
       "      <td>0.493763</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.474935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.347500</td>\n",
       "      <td>1.924666</td>\n",
       "      <td>1.988716</td>\n",
       "      <td>0.628818</td>\n",
       "      <td>0.177210</td>\n",
       "      <td>1.037131</td>\n",
       "      <td>0.473810</td>\n",
       "      <td>0.476316</td>\n",
       "      <td>0.462803</td>\n",
       "      <td>0.455431</td>\n",
       "      <td>30</td>\n",
       "      <td>0.476316</td>\n",
       "      <td>0.462803</td>\n",
       "      <td>0.455431</td>\n",
       "      <td>0.495049</td>\n",
       "      <td>0.473810</td>\n",
       "      <td>0.471699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.265100</td>\n",
       "      <td>1.873951</td>\n",
       "      <td>1.956969</td>\n",
       "      <td>0.614132</td>\n",
       "      <td>0.175877</td>\n",
       "      <td>1.045138</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.472777</td>\n",
       "      <td>0.458090</td>\n",
       "      <td>0.451198</td>\n",
       "      <td>30</td>\n",
       "      <td>0.472777</td>\n",
       "      <td>0.458090</td>\n",
       "      <td>0.451198</td>\n",
       "      <td>0.491183</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.467935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.193600</td>\n",
       "      <td>1.847952</td>\n",
       "      <td>1.970874</td>\n",
       "      <td>0.602053</td>\n",
       "      <td>0.175777</td>\n",
       "      <td>1.015710</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.475758</td>\n",
       "      <td>0.460522</td>\n",
       "      <td>0.457497</td>\n",
       "      <td>30</td>\n",
       "      <td>0.475758</td>\n",
       "      <td>0.460522</td>\n",
       "      <td>0.457497</td>\n",
       "      <td>0.496998</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.477289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.130500</td>\n",
       "      <td>1.838208</td>\n",
       "      <td>2.005051</td>\n",
       "      <td>0.594189</td>\n",
       "      <td>0.173931</td>\n",
       "      <td>1.004683</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.464979</td>\n",
       "      <td>0.448909</td>\n",
       "      <td>0.444738</td>\n",
       "      <td>30</td>\n",
       "      <td>0.464979</td>\n",
       "      <td>0.448909</td>\n",
       "      <td>0.444738</td>\n",
       "      <td>0.480645</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.461993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.076700</td>\n",
       "      <td>1.824765</td>\n",
       "      <td>2.022133</td>\n",
       "      <td>0.585387</td>\n",
       "      <td>0.171580</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.473996</td>\n",
       "      <td>0.459307</td>\n",
       "      <td>0.455108</td>\n",
       "      <td>30</td>\n",
       "      <td>0.473996</td>\n",
       "      <td>0.459307</td>\n",
       "      <td>0.455108</td>\n",
       "      <td>0.490879</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.472074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.027500</td>\n",
       "      <td>1.812703</td>\n",
       "      <td>2.029423</td>\n",
       "      <td>0.576689</td>\n",
       "      <td>0.171063</td>\n",
       "      <td>1.011063</td>\n",
       "      <td>0.473810</td>\n",
       "      <td>0.484922</td>\n",
       "      <td>0.461749</td>\n",
       "      <td>0.458940</td>\n",
       "      <td>30</td>\n",
       "      <td>0.484922</td>\n",
       "      <td>0.461749</td>\n",
       "      <td>0.458940</td>\n",
       "      <td>0.493889</td>\n",
       "      <td>0.473810</td>\n",
       "      <td>0.472068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.986700</td>\n",
       "      <td>1.810633</td>\n",
       "      <td>2.053289</td>\n",
       "      <td>0.571118</td>\n",
       "      <td>0.170199</td>\n",
       "      <td>1.008512</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.473648</td>\n",
       "      <td>0.454184</td>\n",
       "      <td>0.450758</td>\n",
       "      <td>30</td>\n",
       "      <td>0.473648</td>\n",
       "      <td>0.454184</td>\n",
       "      <td>0.450758</td>\n",
       "      <td>0.486895</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.465430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.952100</td>\n",
       "      <td>1.800661</td>\n",
       "      <td>2.055523</td>\n",
       "      <td>0.567968</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>1.002933</td>\n",
       "      <td>0.473810</td>\n",
       "      <td>0.467311</td>\n",
       "      <td>0.455880</td>\n",
       "      <td>0.450379</td>\n",
       "      <td>30</td>\n",
       "      <td>0.467311</td>\n",
       "      <td>0.455880</td>\n",
       "      <td>0.450379</td>\n",
       "      <td>0.484703</td>\n",
       "      <td>0.473810</td>\n",
       "      <td>0.467727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>1.803810</td>\n",
       "      <td>2.082116</td>\n",
       "      <td>0.563032</td>\n",
       "      <td>0.169391</td>\n",
       "      <td>0.997090</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.479927</td>\n",
       "      <td>0.462984</td>\n",
       "      <td>0.457926</td>\n",
       "      <td>30</td>\n",
       "      <td>0.479927</td>\n",
       "      <td>0.462984</td>\n",
       "      <td>0.457926</td>\n",
       "      <td>0.497477</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.476243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>1.805962</td>\n",
       "      <td>2.097313</td>\n",
       "      <td>0.560962</td>\n",
       "      <td>0.168768</td>\n",
       "      <td>1.000256</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.470253</td>\n",
       "      <td>0.461273</td>\n",
       "      <td>0.454753</td>\n",
       "      <td>30</td>\n",
       "      <td>0.470253</td>\n",
       "      <td>0.461273</td>\n",
       "      <td>0.454753</td>\n",
       "      <td>0.488939</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.473356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>1.800827</td>\n",
       "      <td>2.097328</td>\n",
       "      <td>0.558842</td>\n",
       "      <td>0.168098</td>\n",
       "      <td>0.997304</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.472025</td>\n",
       "      <td>0.462574</td>\n",
       "      <td>0.454898</td>\n",
       "      <td>30</td>\n",
       "      <td>0.472025</td>\n",
       "      <td>0.462574</td>\n",
       "      <td>0.454898</td>\n",
       "      <td>0.490581</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.473140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 training complete\n",
      "Stage 2 validation: {'eval_loss': 1.7655931154886881, 'eval_classification_loss': 1.9708740739373578, 'eval_feature_distillation_loss': 0.5567482935530799, 'eval_attention_distillation_loss': 0.1755822612061387, 'eval_logits_distillation_loss': 1.0157101900095031, 'eval_accuracy': 0.48095238095238096, 'eval_precision': 0.4757577308312602, 'eval_recall': 0.46052232400478016, 'eval_f1': 0.4574965226279983, 'eval_num_classes': 30, 'eval_macro_precision': 0.4757577308312602, 'eval_macro_recall': 0.46052232400478016, 'eval_macro_f1': 0.4574965226279983, 'eval_weighted_precision': 0.4969976351068788, 'eval_weighted_recall': 0.48095238095238096, 'eval_weighted_f1': 0.47728910952628356, 'epoch': 19.81807372175981}\n",
      "OUTPUT_DIR set to final student: ./contextual_bridge_runs_20250830_111437/stage2_small_to_tiny\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: Initialize + Train Tiny with Frozen Assistant\n",
    "# Clean up Stage 1 objects to free GPU memory before constructing Stage 2 framework\n",
    "\n",
    "from tri_model_distillation import make_metrics_fn\n",
    "if torch.cuda.is_available():\n",
    "    import gc\n",
    "    if not SKIP_STAGE1:\n",
    "        del stage1_trainer, stage1_framework\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    print('CUDA memory cache cleared before Stage 2 initialization.')\n",
    "\n",
    "stage2_framework = TriModelDistillationFramework(\n",
    "    config=stage2_config,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 16 if torch.cuda.is_available() else 8  # keep effective batch similar\n",
    "effective_batch = per_device_train_batch_size * gradient_accumulation_steps\n",
    "train_loader2, val_loader2, test_loader2 = create_data_loaders(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    image_processor=stage2_framework.image_processor,\n",
    "    label2id=label2id,\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    num_frames=stage2_config.num_frames,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "stage2_args = stage2_config.to_training_args(\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=stage2_epochs,\n",
    "    warmup_steps=stage2_warmup,\n",
    "    learning_rate=1e-4,   \n",
    "    eval_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=20,\n",
    "    output_dir=STAGE2_OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=['tensorboard', 'wandb'],\n",
    "    logging_dir=f'{STAGE2_OUTPUT_DIR}/logs',\n",
    ")\n",
    "metrics_fn = make_metrics_fn(\"multiclass\")\n",
    "\n",
    "stage2_trainer = TriModelDistillationTrainer(\n",
    "    framework=stage2_framework,\n",
    "    distillation_config=stage2_config,\n",
    "    args=stage2_args,\n",
    "    train_dataset=train_loader2.dataset,\n",
    "    eval_dataset=val_loader2.dataset, \n",
    "    compute_metrics=metrics_fn,\n",
    ")\n",
    "\n",
    "print('Starting Stage 2 training...')\n",
    "stage2_train_result = stage2_trainer.train()\n",
    "print('Stage 2 training complete')\n",
    "\n",
    "stage2_trainer.save_model(STAGE2_OUTPUT_DIR)\n",
    "stage2_val_metrics = stage2_trainer.evaluate(eval_dataset=val_loader2.dataset)\n",
    "print('Stage 2 validation:', stage2_val_metrics)\n",
    "\n",
    "# For downstream evaluation cells\n",
    "OUTPUT_DIR = STAGE2_OUTPUT_DIR\n",
    "framework = stage2_framework\n",
    "val_loader = val_loader2\n",
    "test_loader = test_loader2\n",
    "print('OUTPUT_DIR set to final student:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on the full test set...\n",
      "./contextual_bridge_runs_20250830_111437/stage2_small_to_tiny\n",
      "Loaded 422 samples from processed_dataset\\test.csv\n",
      "\n",
      "Starting inference on 422 test videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|█████████████████████████████████████████| 422/422 [01:08<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Total videos in test set: 422\n",
      "Videos successfully processed: 422\n",
      "Videos skipped (missing/corrupt): 0\n",
      "Top-1 Correct Predictions: 208\n",
      "Top-5 Correct Predictions: 347\n",
      "Top-1 Accuracy: 49.29%\n",
      "Top-5 Accuracy: 82.23%\n",
      "Average inference time per video: 0.014 seconds (69.98 videos/sec)\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     archery       0.62      0.62      0.62        13\n",
      "    baseball       0.57      0.67      0.62        18\n",
      "  basketball       0.42      0.42      0.42        12\n",
      "         bmx       0.43      0.27      0.33        11\n",
      "     bowling       0.38      0.50      0.43        10\n",
      "      boxing       0.25      0.09      0.13        11\n",
      "cheerleading       0.44      0.21      0.29        19\n",
      " discusthrow       0.25      0.50      0.33         4\n",
      "      diving       0.50      0.73      0.59        11\n",
      "    football       0.61      0.70      0.65        20\n",
      "        golf       0.50      0.55      0.52        11\n",
      "  gymnastics       0.33      0.53      0.41        17\n",
      " hammerthrow       0.80      0.50      0.62        16\n",
      "    highjump       0.40      0.25      0.31        16\n",
      "      hockey       0.56      0.64      0.60        14\n",
      "    hurdling       0.20      0.07      0.11        14\n",
      "     javelin       0.35      0.60      0.44        10\n",
      "    longjump       0.20      0.15      0.17        13\n",
      "   polevault       0.15      0.29      0.20         7\n",
      "      rowing       0.73      0.65      0.69        17\n",
      "     running       0.10      0.09      0.10        11\n",
      "     shotput       0.67      0.24      0.35        17\n",
      "     skating       0.54      0.54      0.54        13\n",
      "      skiing       0.71      0.62      0.67        16\n",
      "      soccer       0.59      0.77      0.67        13\n",
      "    swimming       0.87      0.90      0.88        29\n",
      "      tennis       0.62      0.50      0.56        10\n",
      "  volleyball       0.39      0.67      0.49        24\n",
      "      weight       0.38      0.43      0.40         7\n",
      "   wrestling       0.43      0.33      0.38        18\n",
      "\n",
      "    accuracy                           0.49       422\n",
      "   macro avg       0.47      0.47      0.45       422\n",
      "weighted avg       0.50      0.49      0.48       422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace the problematic last cell with this corrected version:\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Starting evaluation on the full test set...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load model directly (not using pipeline)\n",
    "local_model_directory = OUTPUT_DIR\n",
    "print(OUTPUT_DIR)\n",
    "student_model = VideoMAEForVideoClassification.from_pretrained(local_model_directory)\n",
    "processor = VideoMAEImageProcessor.from_pretrained(local_model_directory)\n",
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "def process_video_for_inference(video_path, processor, num_frames=16):\n",
    "    \"\"\"Process video exactly like the training pipeline\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "        \n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if frame_count < num_frames:\n",
    "            cap.release()\n",
    "            return None\n",
    "            \n",
    "        # Sample frames uniformly (same as training)\n",
    "        frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) != num_frames:\n",
    "            return None\n",
    "            \n",
    "        # Process frames using the same processor\n",
    "        inputs = processor(frames, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load test data from CSV\n",
    "def load_test_data_from_csv(csv_file_path, data_root_path):\n",
    "    test_samples = []\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"ERROR: Test CSV file not found at {csv_file_path}\")\n",
    "        return test_samples\n",
    "\n",
    "    with open(csv_file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f.readlines(), 1):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                relative_video_path = parts[0]\n",
    "                true_label_str = parts[1]\n",
    "                full_video_path = os.path.normpath(os.path.join(data_root_path, relative_video_path))\n",
    "                test_samples.append((full_video_path, true_label_str))\n",
    "            elif line.strip():\n",
    "                print(f\"Warning: Malformed line {line_num} in {csv_file_path}: '{line.strip()}'\")\n",
    "                \n",
    "    print(f\"Loaded {len(test_samples)} samples from {csv_file_path}\")\n",
    "    return test_samples\n",
    "\n",
    "# Load test data\n",
    "dataset_root_path = \"processed_dataset\"\n",
    "test_csv_path = os.path.join(dataset_root_path, \"test.csv\")\n",
    "test_data = load_test_data_from_csv(test_csv_path, dataset_root_path)\n",
    "\n",
    "if test_data:\n",
    "    total_videos_processed = 0\n",
    "    videos_skipped = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    top1_correct_predictions = 0\n",
    "    top5_correct_predictions = 0\n",
    "    inference_times = []\n",
    "\n",
    "    print(f\"\\nStarting inference on {len(test_data)} test videos...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (video_path, true_label) in enumerate(tqdm(test_data, desc=\"Processing videos\")):\n",
    "            if not os.path.exists(video_path):\n",
    "                videos_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Process video using the same pipeline as training\n",
    "            inputs = process_video_for_inference(video_path, processor)\n",
    "            if inputs is None:\n",
    "                videos_skipped += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Move inputs to device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Get model predictions\n",
    "                outputs = student_model(**inputs)\n",
    "                logits = outputs.logits[0]  # Remove batch dimension\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                \n",
    "                # Get top-5 predictions\n",
    "                top5_probs, top5_indices = torch.topk(probs, 5)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                inference_times.append(end_time - start_time)\n",
    "                total_videos_processed += 1\n",
    "\n",
    "                # Convert indices to labels using id2label\n",
    "                predicted_labels_top5 = [id2label[idx.item()] for idx in top5_indices]\n",
    "                predicted_label_top1 = predicted_labels_top5[0]\n",
    "\n",
    "                # Store for metrics calculation\n",
    "                predicted_labels.append(predicted_label_top1)\n",
    "                true_labels.append(true_label)\n",
    "\n",
    "                # Calculate top-k accuracy\n",
    "                if predicted_label_top1 == true_label:\n",
    "                    top1_correct_predictions += 1\n",
    "                if true_label in predicted_labels_top5:\n",
    "                    top5_correct_predictions += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during inference for {video_path}: {e}\")\n",
    "                videos_skipped += 1\n",
    "\n",
    "    # Print results\n",
    "    if total_videos_processed > 0:\n",
    "        top1_accuracy = (top1_correct_predictions / total_videos_processed) * 100\n",
    "        top5_accuracy = (top5_correct_predictions / total_videos_processed) * 100\n",
    "        avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "        fps = 1.0 / avg_inference_time if avg_inference_time > 0 else float('inf')\n",
    "\n",
    "        print(\"\\n--- Evaluation Complete ---\")\n",
    "        print(f\"Total videos in test set: {len(test_data)}\")\n",
    "        print(f\"Videos successfully processed: {total_videos_processed}\")\n",
    "        print(f\"Videos skipped (missing/corrupt): {videos_skipped}\")\n",
    "        print(f\"Top-1 Correct Predictions: {top1_correct_predictions}\")\n",
    "        print(f\"Top-5 Correct Predictions: {top5_correct_predictions}\")\n",
    "        print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
    "        print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
    "        print(f\"Average inference time per video: {avg_inference_time:.3f} seconds ({fps:.2f} videos/sec)\")\n",
    "\n",
    "        # Classification report\n",
    "        if len(predicted_labels) == len(true_labels) and len(true_labels) > 0:\n",
    "            print(\"\\nDetailed Classification Report:\")\n",
    "            print(classification_report(true_labels, predicted_labels, labels=list(label2id.keys()), zero_division=0))\n",
    "    else:\n",
    "        print(\"No videos were processed successfully.\")\n",
    "else:\n",
    "    print(\"No test data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Two-Stage Contextual Bridge Distillation — Sports-in-the-Wild\n",
    "\n",
    "This notebook now implements a **sequential 2-stage knowledge bridge** instead of training all three models jointly. The idea:\n",
    "\n",
    "```\n",
    "Teacher (Base)  ──►  Stage 1: Train Small  ──►  Stage 2: Small (frozen) teaches Tiny\n",
    "        (global rich knowledge)              (domain-adapted bridge)         (final efficient student)\n",
    "```\n",
    "\n",
    "Why a contextual bridge?\n",
    "- The direct gap Teacher(Base) → Student(Tiny) can be large (capacity + representation mismatch).\n",
    "- First adapting an intermediate (Small) yields a *domain-specialized assistant*.\n",
    "- Then the Tiny model learns from both: retained high-level signal (Teacher) + distilled, compressed domain signal (Assistant).\n",
    "\n",
    "Outcome: Better stability + improved tiny accuracy vs single-hop distillation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Flow Overview\n",
    "\n",
    "We run **two independent training jobs** inside one notebook:\n",
    "\n",
    "### Stage 1: Teacher → Assistant (Train ViT-Small)\n",
    "Goal: Produce a strong, domain-adapted SMALL model that will act as a frozen assistant in Stage 2.\n",
    "\n",
    "Configuration principles:\n",
    "- Student model = `videomae-small` (becomes the assistant later)\n",
    "- Assistant influence weights = 0 (no assistant yet)\n",
    "- Only Teacher → Student logits (and optionally features) KD\n",
    "\n",
    "### Stage 2: Assistant → Student (Train ViT-Tiny)\n",
    "Goal: Train the TINY model using BOTH the frozen Small (assistant) and the original Teacher.\n",
    "\n",
    "Configuration principles:\n",
    "- Assistant model path = checkpoint directory produced in Stage 1\n",
    "- Teacher still provides a small stabilizing signal\n",
    "- Assistant has higher logits weight (primary mentor)\n",
    "\n",
    "### Advantages of This Design\n",
    "- Reduces representational jump distance\n",
    "- Lets the Assistant internalize domain specifics before mentoring Tiny\n",
    "- Often yields +accuracy versus a direct Teacher→Tiny pipeline\n",
    "\n",
    "Proceed through sections in order. Skip Stage 1 only if you already have a trained Small checkpoint you want to reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\order\\.conda\\envs\\videomae\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n",
      "CUDA available: True\n",
      "torch: 2.1.0+cu118 cuda: 11.8\n",
      "torchvision: 0.16.0+cu118\n",
      "pytorchvideo: 0.1.5\n",
      "has functional_tensor: True\n"
     ]
    }
   ],
   "source": [
    "# Environment / Common Imports\n",
    "import os, json, torch\n",
    "import torchvision, pytorchvideo, transformers\n",
    "from datetime import datetime\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from tri_model_distillation.config import TriModelConfig\n",
    "from tri_model_distillation.models import TriModelDistillationFramework\n",
    "from tri_model_distillation.trainer import TriModelDistillationTrainer, compute_video_classification_metrics\n",
    "from tri_model_distillation.utils import (\n",
    "    setup_logging, load_label_mappings, create_data_loaders,\n",
    ")\n",
    "\n",
    "print(torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"pytorchvideo:\", pytorchvideo.__version__)\n",
    "print(\"has functional_tensor:\", hasattr(__import__('torchvision.transforms', fromlist=['']), 'functional_tensor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 30 classes\n",
      "['archery', 'baseball', 'basketball', 'bmx', 'bowling', 'boxing', 'cheerleading', 'discusthrow', 'diving', 'football', 'golf', 'gymnastics', 'hammerthrow', 'highjump', 'hockey', 'hurdling', 'javelin', 'longjump', 'polevault', 'rowing', 'running', 'shotput', 'skating', 'skiing', 'soccer', 'swimming', 'tennis', 'volleyball', 'weight', 'wrestling'] ...\n"
     ]
    }
   ],
   "source": [
    "# Dataset + Label Mapping\n",
    "DATASET_ROOT = 'processed_dataset'\n",
    "BASE_RUN_DIR = f\"./contextual_bridge_runs_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "STAGE1_OUTPUT_DIR = f\"{BASE_RUN_DIR}/stage1_teacher_to_small\"\n",
    "STAGE2_OUTPUT_DIR = f\"{BASE_RUN_DIR}/stage2_small_to_tiny\"\n",
    "os.makedirs(STAGE1_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(STAGE2_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "label2id, id2label = load_label_mappings(dataset_root=DATASET_ROOT, train_csv='train.csv', classification_type='multiclass')\n",
    "num_labels = len(label2id)\n",
    "print(f\"Detected {num_labels} classes\")\n",
    "print(list(label2id.keys())[:30], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 config ready (logits-only KD; features/attn disabled)\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 Configuration (Teacher + Pretrained Assistant → Train Small Student)\n",
    "# Dual supervision with ONLY logits KD (features/attentions disabled to save memory)\n",
    "pretrained_small_ckpt = 'mitegvg/videomae-small-finetuned-ssv2-finetuned-sports-videos-in-the-wild'\n",
    "\n",
    "stage1_config = TriModelConfig(\n",
    "    classification_type='multiclass',\n",
    "    num_labels=num_labels,\n",
    "    teacher_model_name='mitegvg/videomae-base-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "    assistant_model_name=pretrained_small_ckpt,\n",
    "    student_model_name=pretrained_small_ckpt,\n",
    "    temperature=4.0,\n",
    "    logits_temperature=4.0,\n",
    "    teacher_logits_weight=1.0,\n",
    "    assistant_logits_weight=0.5,\n",
    "    classification_loss_weight=1.0,\n",
    "    logits_distillation_weight=0.35,\n",
    "    hidden_layers_to_align=[],\n",
    "    feature_distillation_weight=0.0,\n",
    "    attention_distillation_weight=0.0,\n",
    "    use_pretrained_student=True,\n",
    "    num_frames=16,\n",
    "    apply_defaults=False,  # NEW: prevent auto override adding hidden/attn needs\n",
    ")\n",
    "\n",
    "# Memory safety knobs\n",
    "TOTAL_TRAIN_SAMPLES = 3364\n",
    "per_device_train_batch_size = 2 if torch.cuda.is_available() else 2  # reduced\n",
    "gradient_accumulation_steps = 16 if torch.cuda.is_available() else 8  # keep effective batch similar\n",
    "effective_batch = per_device_train_batch_size * gradient_accumulation_steps\n",
    "stage1_epochs = 12\n",
    "steps_per_epoch = TOTAL_TRAIN_SAMPLES // effective_batch\n",
    "stage1_total_steps = steps_per_epoch * stage1_epochs\n",
    "stage1_warmup = min(500, int(0.1 * stage1_total_steps))\n",
    "\n",
    "print('Stage 1 config ready (logits-only KD; features/attn disabled)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Stage 1 checkpoint detected in ./contextual_bridge_runs_20250824_214420/stage1_teacher_to_small. Stage 1 training will be skipped.\n",
      "(Delete or rename the directory to force retraining.)\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 Skip Logic — Detect existing checkpoint to optionally skip Stage 1 training\n",
    "import os, glob\n",
    "stage1_checkpoint_exists = any(\n",
    "    os.path.exists(os.path.join(STAGE1_OUTPUT_DIR, fname))\n",
    "    for fname in ['pytorch_model.bin', 'model.safetensors', 'config.json']\n",
    ")\n",
    "if not stage1_checkpoint_exists:\n",
    "    for sub in glob.glob(os.path.join(STAGE1_OUTPUT_DIR, 'checkpoint-*')):\n",
    "        if os.path.exists(os.path.join(sub, 'pytorch_model.bin')) or os.path.exists(os.path.join(sub, 'model.safetensors')):\n",
    "            stage1_checkpoint_exists = True\n",
    "            break\n",
    "\n",
    "SKIP_STAGE1 = stage1_checkpoint_exists\n",
    "if SKIP_STAGE1:\n",
    "    print(f\"Existing Stage 1 checkpoint detected in {STAGE1_OUTPUT_DIR}. Stage 1 training will be skipped.\")\n",
    "    print(\"(Delete or rename the directory to force retraining.)\")\n",
    "else:\n",
    "    print(\"No existing Stage 1 checkpoint found. Stage 1 training will run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Initialize Framework + Dataloaders\n",
    "if SKIP_STAGE1:\n",
    "    print('Stage 1 skipped: loading assistant from existing checkpoint for Stage 2.')\n",
    "else:\n",
    "    setup_logging()\n",
    "\n",
    "    stage1_framework = TriModelDistillationFramework(\n",
    "        config=stage1_config,\n",
    "        num_labels=num_labels,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "    )\n",
    "\n",
    "    def _count_params(m):\n",
    "        return sum(p.numel() for p in m.parameters() if p.requires_grad), sum(p.numel() for p in m.parameters() if not p.requires_grad)\n",
    "\n",
    "    tr_s, fr_s = _count_params(stage1_framework.student_model)\n",
    "    tr_t, fr_t = _count_params(stage1_framework.teacher_model)\n",
    "    tr_a, fr_a = _count_params(stage1_framework.assistant_model)\n",
    "    print(f\"Teacher trainable {tr_t:,} frozen {fr_t:,}\")\n",
    "    print(f\"Assistant trainable {tr_a:,} frozen {fr_a:,}\")\n",
    "    print(f\"Student trainable {tr_s:,} frozen {fr_s:,}\")\n",
    "    print('Need hidden states:', getattr(stage1_framework,'_need_hidden',True), 'Need attentions:', getattr(stage1_framework,'_need_attn',True))\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        dataset_root=DATASET_ROOT,\n",
    "        image_processor=stage1_framework.image_processor,\n",
    "        label2id=label2id,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        num_frames=stage1_config.num_frames,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    print('Data loaders ready')\n",
    "\n",
    "    # Dry-run memory probe\n",
    "    if torch.cuda.is_available():\n",
    "        import gc\n",
    "        batch = next(iter(train_loader))\n",
    "        batch = {k: v.to('cuda') if hasattr(v,'to') else v for k,v in batch.items()}\n",
    "        torch.cuda.empty_cache(); gc.collect(); torch.cuda.reset_peak_memory_stats()\n",
    "        out = stage1_framework(pixel_values=batch['pixel_values'], labels=batch['labels'], output_hidden_states=False, output_attentions=False)\n",
    "        loss_probe = out['student'].logits.mean(); loss_probe.backward();\n",
    "        peak = torch.cuda.max_memory_allocated()/1024/1024\n",
    "        print(f\"Dry-run peak MB: {peak:.1f}\")\n",
    "        del out, batch, loss_probe; torch.cuda.empty_cache(); gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Stage 1 training; will use existing checkpoint as assistant in Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Train Small (Assistant-to-be)\n",
    "if SKIP_STAGE1:\n",
    "    print('Skipping Stage 1 training; will use existing checkpoint as assistant in Stage 2.')\n",
    "    stage1_trainer = None\n",
    "    stage1_framework = None\n",
    "else:\n",
    "    # Enable gradient checkpointing for student to reduce activation memory\n",
    "    if hasattr(stage1_framework.student_model, 'gradient_checkpointing_enable'):\n",
    "        stage1_framework.student_model.gradient_checkpointing_enable()\n",
    "\n",
    "    stage1_args = stage1_config.to_training_args(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=stage1_epochs,\n",
    "        warmup_steps=stage1_warmup,\n",
    "        evaluation_strategy='epoch',\n",
    "        logging_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        save_total_limit=20,\n",
    "        output_dir=STAGE1_OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=['tensorboard'],\n",
    "        logging_dir=f'{STAGE1_OUTPUT_DIR}/logs',\n",
    "    )\n",
    "\n",
    "    stage1_trainer = TriModelDistillationTrainer(\n",
    "        framework=stage1_framework,\n",
    "        distillation_config=stage1_config,\n",
    "        args=stage1_args,\n",
    "        train_dataset=train_loader.dataset,\n",
    "        eval_dataset=val_loader.dataset,\n",
    "        compute_metrics=lambda eval_pred, **kw: compute_video_classification_metrics(eval_pred, classification_type='multiclass'),\n",
    "    )\n",
    "\n",
    "    print('Starting Stage 1 training (logits-only KD, memory optimized)...')\n",
    "    stage1_train_result = stage1_trainer.train()\n",
    "    print('Stage 1 training complete')\n",
    "\n",
    "    stage1_trainer.save_model(STAGE1_OUTPUT_DIR)\n",
    "    stage1_val_metrics = stage1_trainer.evaluate(eval_dataset=val_loader.dataset)\n",
    "    print('Stage 1 validation:', stage1_val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 config ready (full KD: logits + features + attention)\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 Configuration (Assistant → Tiny)\n",
    "# If Stage 1 skipped, we still expect STAGE1_OUTPUT_DIR to already contain a trained small model\n",
    "if SKIP_STAGE1:\n",
    "    assert os.path.exists(os.path.join(STAGE1_OUTPUT_DIR, 'config.json')), 'Expected existing Stage 1 checkpoint missing.'\n",
    "\n",
    "# Stage 2 Configuration (Assistant → Tiny) — full logits + feature + attention KD\n",
    "stage2_config = TriModelConfig(\n",
    "    classification_type='multiclass',\n",
    "    num_labels=num_labels,\n",
    "    teacher_model_name='mitegvg/videomae-base-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "    assistant_model_name=STAGE1_OUTPUT_DIR,  # trained small (assistant)\n",
    "    student_model_name='mitegvg/videomae-tiny-finetuned-kinetics-finetuned-sports-videos-in-the-wild',\n",
    "    # Distillation temperatures\n",
    "    temperature=4.0,\n",
    "    logits_temperature=4.0,\n",
    "    # Logits KD weighting\n",
    "    teacher_logits_weight=0.0,\n",
    "    assistant_logits_weight=1.0,\n",
    "    logits_distillation_weight=0.15,\n",
    "    # Supervised classification loss\n",
    "    classification_loss_weight=0.70,\n",
    "    # Representation alignment\n",
    "    align_hidden_states=True,\n",
    "    align_attention_maps=True,\n",
    "    hidden_layers_to_align=[],  # spaced to reduce redundancy & memory\n",
    "    feature_distillation_weight=0.0,\n",
    "    attention_distillation_weight=0.0,\n",
    "    # (If your loss uses these per-source feature weights)\n",
    "    teacher_feature_weight=0.0,\n",
    "    assistant_feature_weight=0.0,\n",
    "    # Student init\n",
    "    use_pretrained_student=True,\n",
    "    num_frames=16,\n",
    ")\n",
    "\n",
    "stage2_epochs = 20\n",
    "stage2_total_steps = steps_per_epoch * stage2_epochs\n",
    "stage2_warmup = min(500, int(0.1 * stage2_total_steps))\n",
    "print('Stage 2 config ready (full KD: logits + features + attention)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cache cleared before Stage 2 initialization.\n",
      "[TriModelConfig] Active components => logits:True features:False attn:False\n",
      "[TriModelConfig] Using legacy eval_strategy fallback.\n",
      "Starting Stage 2 training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mite_gvg (mitegvg) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\BIRKBECK\\REPOS\\videomae-base-finetuned-sports-in-the-wild\\wandb\\run-20250824_214458-z9tmz5iw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitegvg/huggingface/runs/z9tmz5iw' target=\"_blank\">./contextual_bridge_runs_20250824_214420/stage2_small_to_tiny</a></strong> to <a href='https://wandb.ai/mitegvg/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitegvg/huggingface' target=\"_blank\">https://wandb.ai/mitegvg/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitegvg/huggingface/runs/z9tmz5iw' target=\"_blank\">https://wandb.ai/mitegvg/huggingface/runs/z9tmz5iw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2100/2100 5:35:50, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 training complete\n",
      "Stage 2 validation: {'eval_loss': 3.6225275925227574, 'eval_classification_loss': 1.8903797278535508, 'eval_feature_distillation_loss': 0.0, 'eval_attention_distillation_loss': 0.0, 'eval_logits_distillation_loss': 11.547651935759045, 'eval_accuracy': 0.4880952380952381, 'eval_precision_macro': 0.47579232615223327, 'eval_precision_micro': 0.4880952380952381, 'eval_precision_weighted': 0.4893833281175718, 'eval_recall_macro': 0.4707976831771982, 'eval_recall_micro': 0.4880952380952381, 'eval_recall_weighted': 0.4880952380952381, 'eval_f1_macro': 0.4611007316882032, 'eval_f1_micro': 0.4880952380952381, 'eval_f1_weighted': 0.476842110187112, 'eval_roc_auc_ovr': 0.9208079792034629, 'eval_roc_auc_ovo': 0.9200895195307596, 'eval_cohen_kappa': 0.4683638204800622, 'eval_balanced_accuracy': 0.4707976831771982, 'eval_top1_accuracy': 0.4880952380952381, 'eval_precision': 0.47579232615223327, 'eval_recall': 0.4707976831771982, 'eval_f1_score': 0.4611007316882032, 'eval_class_0_accuracy': 0.4166666666666667, 'eval_class_0_f1': 0.3333333333333333, 'eval_class_1_accuracy': 0.23529411764705882, 'eval_class_1_f1': 0.26666666666666666, 'eval_class_2_accuracy': 0.375, 'eval_class_2_f1': 0.5, 'eval_class_3_accuracy': 0.3, 'eval_class_3_f1': 0.41379310344827586, 'eval_class_4_accuracy': 0.6470588235294118, 'eval_class_4_f1': 0.6470588235294118, 'eval_class_5_accuracy': 0.3333333333333333, 'eval_class_5_f1': 0.4444444444444444, 'eval_class_6_accuracy': 0.5789473684210527, 'eval_class_6_f1': 0.5789473684210527, 'eval_class_7_accuracy': 0.3333333333333333, 'eval_class_7_f1': 0.26666666666666666, 'eval_class_8_accuracy': 0.6875, 'eval_class_8_f1': 0.7333333333333333, 'eval_class_9_accuracy': 0.3333333333333333, 'eval_class_9_f1': 0.4, 'eval_class_10_accuracy': 0.6666666666666666, 'eval_class_10_f1': 0.6666666666666666, 'eval_class_11_accuracy': 0.6470588235294118, 'eval_class_11_f1': 0.6666666666666666, 'eval_class_12_accuracy': 0.375, 'eval_class_12_f1': 0.2608695652173913, 'eval_class_13_accuracy': 0.4, 'eval_class_13_f1': 0.34782608695652173, 'eval_class_14_accuracy': 0.7777777777777778, 'eval_class_14_f1': 0.6363636363636364, 'eval_class_15_accuracy': 0.5, 'eval_class_15_f1': 0.5333333333333333, 'eval_class_16_accuracy': 0.2222222222222222, 'eval_class_16_f1': 0.21052631578947367, 'eval_class_17_accuracy': 0.07692307692307693, 'eval_class_17_f1': 0.09090909090909091, 'eval_class_18_accuracy': 0.4166666666666667, 'eval_class_18_f1': 0.4, 'eval_class_19_accuracy': 0.5555555555555556, 'eval_class_19_f1': 0.5555555555555556, 'eval_avg_class_accuracy': 0.4439168882802783, 'eval_avg_class_f1': 0.44764803286507593, 'eval_top5_accuracy': 0.8095238095238095, 'eval_confusion_matrix_trace': 205.0, 'eval_confusion_matrix_total': 420.0, 'eval_num_samples': 420, 'eval_num_classes': 30, 'eval_classification_type': 'multiclass', 'epoch': 19.81807372175981}\n",
      "OUTPUT_DIR set to final student: ./contextual_bridge_runs_20250824_214420/stage2_small_to_tiny\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: Initialize + Train Tiny with Frozen Assistant\n",
    "# Clean up Stage 1 objects to free GPU memory before constructing Stage 2 framework\n",
    "if torch.cuda.is_available():\n",
    "    import gc\n",
    "    if not SKIP_STAGE1:\n",
    "        del stage1_trainer, stage1_framework\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    print('CUDA memory cache cleared before Stage 2 initialization.')\n",
    "\n",
    "stage2_framework = TriModelDistillationFramework(\n",
    "    config=stage2_config,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "train_loader2, val_loader2, test_loader2 = create_data_loaders(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    image_processor=stage2_framework.image_processor,\n",
    "    label2id=label2id,\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    num_frames=stage2_config.num_frames,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "stage2_args = stage2_config.to_training_args(\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=stage2_epochs,\n",
    "    warmup_steps=stage2_warmup,\n",
    "    eval_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=20,\n",
    "    output_dir=STAGE2_OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=['tensorboard', 'wandb'],\n",
    "    logging_dir=f'{STAGE2_OUTPUT_DIR}/logs',\n",
    ")\n",
    "\n",
    "stage2_trainer = TriModelDistillationTrainer(\n",
    "    framework=stage2_framework,\n",
    "    distillation_config=stage2_config,\n",
    "    args=stage2_args,\n",
    "    train_dataset=train_loader2.dataset,\n",
    "    eval_dataset=val_loader2.dataset,\n",
    "    compute_metrics=lambda eval_pred, **kw: compute_video_classification_metrics(eval_pred, classification_type='multiclass'),\n",
    ")\n",
    "\n",
    "print('Starting Stage 2 training...')\n",
    "stage2_train_result = stage2_trainer.train()\n",
    "print('Stage 2 training complete')\n",
    "\n",
    "stage2_trainer.save_model(STAGE2_OUTPUT_DIR)\n",
    "stage2_val_metrics = stage2_trainer.evaluate(eval_dataset=val_loader2.dataset)\n",
    "print('Stage 2 validation:', stage2_val_metrics)\n",
    "\n",
    "# For downstream evaluation cells\n",
    "OUTPUT_DIR = STAGE2_OUTPUT_DIR\n",
    "framework = stage2_framework\n",
    "val_loader = val_loader2\n",
    "test_loader = test_loader2\n",
    "print('OUTPUT_DIR set to final student:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2 Rationale\n",
    "The trained Small model now serves as a **domain-adapted bridge**. In Stage 2 we:\n",
    "- Freeze the Small checkpoint (loaded via its output directory)\n",
    "- Keep a light stabilizing signal from the original Base teacher (lower weight)\n",
    "- Emphasize logits distillation from the Assistant (higher weight)\n",
    "\n",
    "Tuning tips:\n",
    "- If Tiny underfits early: increase `assistant_logits_weight` or `logits_temperature`\n",
    "- If overfitting: reduce `classification_loss_weight` slightly or add light feature distillation\n",
    "- If training unstable: raise `teacher_logits_weight` to 0.4–0.5 for extra regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on the full test set...\n",
      "./contextual_bridge_runs_20250824_214420/stage2_small_to_tiny\n",
      "Loaded 422 samples from processed_dataset\\test.csv\n",
      "\n",
      "Starting inference on 422 test videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|████████████████████████████| 422/422 [01:28<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Total videos in test set: 422\n",
      "Videos successfully processed: 422\n",
      "Videos skipped (missing/corrupt): 0\n",
      "Top-1 Correct Predictions: 205\n",
      "Top-5 Correct Predictions: 344\n",
      "Top-1 Accuracy: 48.58%\n",
      "Top-5 Accuracy: 81.52%\n",
      "Average inference time per video: 0.012 seconds (86.89 videos/sec)\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     archery       0.53      0.69      0.60        13\n",
      "    baseball       0.56      0.50      0.53        18\n",
      "  basketball       0.75      0.25      0.38        12\n",
      "         bmx       0.60      0.27      0.38        11\n",
      "     bowling       0.47      0.80      0.59        10\n",
      "      boxing       0.00      0.00      0.00        11\n",
      "cheerleading       0.60      0.32      0.41        19\n",
      " discusthrow       0.20      0.50      0.29         4\n",
      "      diving       0.70      0.64      0.67        11\n",
      "    football       0.48      0.55      0.51        20\n",
      "        golf       0.50      0.64      0.56        11\n",
      "  gymnastics       0.34      0.59      0.43        17\n",
      " hammerthrow       0.50      0.56      0.53        16\n",
      "    highjump       0.33      0.12      0.18        16\n",
      "      hockey       0.59      0.71      0.65        14\n",
      "    hurdling       0.29      0.14      0.19        14\n",
      "     javelin       0.38      0.60      0.46        10\n",
      "    longjump       0.25      0.23      0.24        13\n",
      "   polevault       0.20      0.43      0.27         7\n",
      "      rowing       0.57      0.47      0.52        17\n",
      "     running       0.00      0.00      0.00        11\n",
      "     shotput       0.67      0.12      0.20        17\n",
      "     skating       0.58      0.54      0.56        13\n",
      "      skiing       0.71      0.62      0.67        16\n",
      "      soccer       0.47      0.69      0.56        13\n",
      "    swimming       0.80      0.97      0.88        29\n",
      "      tennis       0.43      0.60      0.50        10\n",
      "  volleyball       0.47      0.58      0.52        24\n",
      "      weight       0.30      0.43      0.35         7\n",
      "   wrestling       0.42      0.44      0.43        18\n",
      "\n",
      "    accuracy                           0.49       422\n",
      "   macro avg       0.46      0.47      0.43       422\n",
      "weighted avg       0.49      0.49      0.46       422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace the problematic last cell with this corrected version:\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Starting evaluation on the full test set...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load model directly (not using pipeline)\n",
    "local_model_directory = OUTPUT_DIR\n",
    "print(OUTPUT_DIR)\n",
    "student_model = VideoMAEForVideoClassification.from_pretrained(local_model_directory)\n",
    "processor = VideoMAEImageProcessor.from_pretrained(local_model_directory)\n",
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "def process_video_for_inference(video_path, processor, num_frames=16):\n",
    "    \"\"\"Process video exactly like the training pipeline\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "        \n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if frame_count < num_frames:\n",
    "            cap.release()\n",
    "            return None\n",
    "            \n",
    "        # Sample frames uniformly (same as training)\n",
    "        frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) != num_frames:\n",
    "            return None\n",
    "            \n",
    "        # Process frames using the same processor\n",
    "        inputs = processor(frames, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load test data from CSV\n",
    "def load_test_data_from_csv(csv_file_path, data_root_path):\n",
    "    test_samples = []\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"ERROR: Test CSV file not found at {csv_file_path}\")\n",
    "        return test_samples\n",
    "\n",
    "    with open(csv_file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f.readlines(), 1):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                relative_video_path = parts[0]\n",
    "                true_label_str = parts[1]\n",
    "                full_video_path = os.path.normpath(os.path.join(data_root_path, relative_video_path))\n",
    "                test_samples.append((full_video_path, true_label_str))\n",
    "            elif line.strip():\n",
    "                print(f\"Warning: Malformed line {line_num} in {csv_file_path}: '{line.strip()}'\")\n",
    "                \n",
    "    print(f\"Loaded {len(test_samples)} samples from {csv_file_path}\")\n",
    "    return test_samples\n",
    "\n",
    "# Load test data\n",
    "dataset_root_path = \"processed_dataset\"\n",
    "test_csv_path = os.path.join(dataset_root_path, \"test.csv\")\n",
    "test_data = load_test_data_from_csv(test_csv_path, dataset_root_path)\n",
    "\n",
    "if test_data:\n",
    "    total_videos_processed = 0\n",
    "    videos_skipped = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    top1_correct_predictions = 0\n",
    "    top5_correct_predictions = 0\n",
    "    inference_times = []\n",
    "\n",
    "    print(f\"\\nStarting inference on {len(test_data)} test videos...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (video_path, true_label) in enumerate(tqdm(test_data, desc=\"Processing videos\")):\n",
    "            if not os.path.exists(video_path):\n",
    "                videos_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Process video using the same pipeline as training\n",
    "            inputs = process_video_for_inference(video_path, processor)\n",
    "            if inputs is None:\n",
    "                videos_skipped += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Move inputs to device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Get model predictions\n",
    "                outputs = student_model(**inputs)\n",
    "                logits = outputs.logits[0]  # Remove batch dimension\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                \n",
    "                # Get top-5 predictions\n",
    "                top5_probs, top5_indices = torch.topk(probs, 5)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                inference_times.append(end_time - start_time)\n",
    "                total_videos_processed += 1\n",
    "\n",
    "                # Convert indices to labels using id2label\n",
    "                predicted_labels_top5 = [id2label[idx.item()] for idx in top5_indices]\n",
    "                predicted_label_top1 = predicted_labels_top5[0]\n",
    "\n",
    "                # Store for metrics calculation\n",
    "                predicted_labels.append(predicted_label_top1)\n",
    "                true_labels.append(true_label)\n",
    "\n",
    "                # Calculate top-k accuracy\n",
    "                if predicted_label_top1 == true_label:\n",
    "                    top1_correct_predictions += 1\n",
    "                if true_label in predicted_labels_top5:\n",
    "                    top5_correct_predictions += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during inference for {video_path}: {e}\")\n",
    "                videos_skipped += 1\n",
    "\n",
    "    # Print results\n",
    "    if total_videos_processed > 0:\n",
    "        top1_accuracy = (top1_correct_predictions / total_videos_processed) * 100\n",
    "        top5_accuracy = (top5_correct_predictions / total_videos_processed) * 100\n",
    "        avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "        fps = 1.0 / avg_inference_time if avg_inference_time > 0 else float('inf')\n",
    "\n",
    "        print(\"\\n--- Evaluation Complete ---\")\n",
    "        print(f\"Total videos in test set: {len(test_data)}\")\n",
    "        print(f\"Videos successfully processed: {total_videos_processed}\")\n",
    "        print(f\"Videos skipped (missing/corrupt): {videos_skipped}\")\n",
    "        print(f\"Top-1 Correct Predictions: {top1_correct_predictions}\")\n",
    "        print(f\"Top-5 Correct Predictions: {top5_correct_predictions}\")\n",
    "        print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
    "        print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
    "        print(f\"Average inference time per video: {avg_inference_time:.3f} seconds ({fps:.2f} videos/sec)\")\n",
    "\n",
    "        # Classification report\n",
    "        if len(predicted_labels) == len(true_labels) and len(true_labels) > 0:\n",
    "            print(\"\\nDetailed Classification Report:\")\n",
    "            print(classification_report(true_labels, predicted_labels, labels=list(label2id.keys()), zero_division=0))\n",
    "    else:\n",
    "        print(\"No videos were processed successfully.\")\n",
    "else:\n",
    "    print(\"No test data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
